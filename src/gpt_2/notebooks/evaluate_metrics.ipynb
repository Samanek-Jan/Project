{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re, json, csv\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from pymongo import MongoClient\n",
    "from tqdm.notebook import tqdm\n",
    "from pprint import PrettyPrinter\n",
    "from typing import Union, Iterable\n",
    "from bson.objectid import ObjectId\n",
    "from torchmetrics.text.bert import BERTScore\n",
    "from torchmetrics.text.rouge import ROUGEScore\n",
    "from torchmetrics.text.bleu import BLEUScore\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoModelForCausalLM, AutoConfig, AutoTokenizer, pipeline, set_seed\n",
    "from torchmetrics.functional import pairwise_cosine_similarity\n",
    "from torchmetrics.functional.text.bert import _get_precision_recall_f1\n",
    "from functools import partial\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import chain\n",
    "from math import log\n",
    "from multiprocessing import Pool\n",
    "from typing import List\n",
    "from pymongo import MongoClient\n",
    "from typing import Set\n",
    "\n",
    "pprint = PrettyPrinter().pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = MongoClient(\"mongodb://localhost:27017\")[\"cuda_snippets\"]\n",
    "train_db = db[\"train\"]\n",
    "validation_db = db[\"validation\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rouge_score(target_sentences : Iterable[str], pred_senteces : Iterable[str], tokenizer):\n",
    "    tokenizer_fn = lambda x: tokenizer.convert_ids_to_tokens(tokenizer.encode(x))\n",
    "    rouge_metric = ROUGEScore(tokenizer=tokenizer_fn)\n",
    "    # rouge_metric.update(pred_senteces, target_sentences)\n",
    "    return rouge_metric(pred_senteces, target_sentences)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bleu_score(target_sentences : Iterable[str], pred_senteces : Iterable[str], tokenizer):\n",
    "    bleu_metric = BLEUScore(tokenizer=tokenizer)\n",
    "    bleu_metric.update(pred_senteces, [[s] for s in target_sentences])\n",
    "    return bleu_metric.compute()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate BERT score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(a, tokenizer=None):\n",
    "    if tokenizer is not None:\n",
    "        a = tokenizer.encode(a, max_length=512, truncation=True, add_special_tokens=False)\n",
    "    return set(a)\n",
    "\n",
    "def get_idf_dict(arr, tokenizer, nthreads=4):\n",
    "    idf_count = Counter()\n",
    "    num_docs = len(arr)\n",
    "\n",
    "    process_partial = partial(process, tokenizer=tokenizer)\n",
    "\n",
    "    if nthreads > 0:\n",
    "        with Pool(nthreads) as p:\n",
    "            idf_count.update(chain.from_iterable(p.map(process_partial, arr)))\n",
    "    else:\n",
    "        idf_count.update(chain.from_iterable(map(process_partial, arr)))\n",
    "\n",
    "    idf_dict = defaultdict(lambda: log((num_docs + 1) / (1)))\n",
    "    idf_dict.update(\n",
    "        {idx: log((num_docs + 1) / (c + 1)) for (idx, c) in idf_count.items()}\n",
    "    )\n",
    "    return idf_dict\n",
    "\n",
    "class TextDataset:\n",
    "    def __init__(self, target_sentences, pred_sentences) -> None:\n",
    "        self.target_sentences = target_sentences\n",
    "        self.pred_sentences = pred_sentences\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.target_sentences)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.target_sentences[i], self.pred_sentences[i]\n",
    "    \n",
    "class CollateFn:\n",
    "    def __init__(self, tokenizer) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __call__(self, samples : list):\n",
    "        targets, preds = zip(*samples)\n",
    "        \n",
    "        targets_pt = self.tokenizer(targets, max_length=512, truncation=True, return_tensors=\"pt\", add_special_tokens=False, padding=True)\n",
    "        preds_pt = self.tokenizer(preds, max_length=512, truncation=True, return_tensors=\"pt\", add_special_tokens=False, padding=True)\n",
    "\n",
    "        return (targets_pt, targets), (preds_pt, preds)\n",
    "\n",
    "def compute_bert_score(target_sentences : Iterable[str], pred_sentences : Iterable[str], tokenizer, embedd_layer : torch.nn.Embedding, idf_scores : dict = None, batch_size=64):\n",
    "\n",
    "    if idf_scores is None:\n",
    "        idf_scores = get_idf_dict(target_sentences, tokenizer)\n",
    "    precs = []\n",
    "    recals = []\n",
    "    f1s = []\n",
    "    \n",
    "    dataset = TextDataset(target_sentences, pred_sentences)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=CollateFn(tokenizer))\n",
    "    \n",
    "    for i, ((target_pt, target_str), (pred_pt, pred_str)) in enumerate(tqdm(dataloader)):\n",
    "\n",
    "        pred_embedd = embedd_layer(pred_pt[\"input_ids\"]).view((pred_pt[\"input_ids\"].size(0),1,pred_pt[\"input_ids\"].size(1), -1))\n",
    "        target_embedd = embedd_layer(target_pt[\"input_ids\"]).view((target_pt[\"input_ids\"].size(0),1,target_pt[\"input_ids\"].size(1), -1))\n",
    "        \n",
    "        pred_idfs = [tokenizer.encode(sentence, max_length=512, add_special_tokens=False, truncation=True, return_tensors=\"pt\")[0] for sentence in pred_str]\n",
    "        pred_idfs = [torch.Tensor([idf_scores.get(token, 0) for token in sentence]) for sentence in pred_idfs]\n",
    "        pred_idfs = torch.nn.utils.rnn.pad_sequence(pred_idfs, batch_first=True, padding_value=0)\n",
    "        \n",
    "        target_idfs = [tokenizer.encode(sentence, max_length=512, add_special_tokens=False, truncation=True, return_tensors=\"pt\")[0] for sentence in target_str]\n",
    "        target_idfs = [torch.Tensor([idf_scores.get(token, 0) for token in sentence]) for sentence in target_idfs]\n",
    "        target_idfs = torch.nn.utils.rnn.pad_sequence(target_idfs, batch_first=True, padding_value=0)\n",
    "\n",
    "        # pred_idfs = torch.Tensor(\n",
    "        #     [[idf_scores.get(token, 0) for token in sentence] for sentence in pred_str]\n",
    "        # )\n",
    "        \n",
    "        # target_idfs = torch.Tensor(\n",
    "        #     [[idf_scores.get(token, 0) for token in sentence] for sentence in target_str]\n",
    "        # )\n",
    "        \n",
    "        # cos_sims = pairwise_cosine_similarity(target_embedd, pred_embedd)\n",
    "        \n",
    "        # max_sims = torch.max(cos_sims, dim=1)[0]\n",
    "        # sentence_scores.append(sum(max_sims*target_idf)/sum(target_idf))\n",
    "        \n",
    "        prec, res, f1 = _get_precision_recall_f1(pred_embedd, target_embedd, pred_idfs, target_idfs)\n",
    "        precs.append(torch.mean(prec))\n",
    "        recals.append(torch.mean(res))\n",
    "        f1s.append(torch.mean(f1))\n",
    "        \n",
    "    return  sum(precs) / len(precs), \\\n",
    "            sum(recals) / len(recals), \\\n",
    "            sum(f1s) / len(f1s)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(self, preds: List[str], targets: List[str]) -> None:\n",
    "        \"\"\"Store predictions/references for computing BERT scores.\n",
    "\n",
    "        It is necessary to store sentences in a tokenized form to ensure the DDP mode working.\n",
    "        \"\"\"\n",
    "        \n",
    "        preds_tokenized_data = self.tokenizer(preds, max_length=1024, truncation=True, return_tensors=\"pt\", padding=True)\n",
    "        preds_dict = {\"input_ids\": preds_tokenized_data[\"input_ids\"], \"attention_mask\": preds_tokenized_data[\"attention_mask\"]}\n",
    "        target_tokenized_data = self.tokenizer(targets, max_length=1024, truncation=True, return_tensors=\"pt\", padding=True)\n",
    "        target_dict = {\"input_ids\": target_tokenized_data[\"input_ids\"], \"attention_mask\": target_tokenized_data[\"attention_mask\"]}\n",
    "\n",
    "        self.preds_input_ids.append(preds_dict[\"input_ids\"])\n",
    "        self.preds_attention_mask.append(preds_dict[\"attention_mask\"])\n",
    "        self.target_input_ids.append(target_dict[\"input_ids\"])\n",
    "        self.target_attention_mask.append(target_dict[\"attention_mask\"])\n",
    "        \n",
    "def compute_bert_score2(target_sentences : Iterable[str], pred_senteces : Iterable[str], tokenizer, model : torch.nn.Module, *args, **kwargs):\n",
    "    BERTScore.update = update\n",
    "    user_forward_fn = lambda model, d: model.get_input_embeddings()(d[\"input_ids\"])\n",
    "    bert_metric = BERTScore(model=model, user_tokenizer=tokenizer, user_forward_fn=user_forward_fn, device=DEVICE, verbose=True, num_layers=0, max_length=1024)\n",
    "    # bert_metric.update = update\n",
    "    output = {}\n",
    "    for key, vals in bert_metric(preds=pred_senteces, targets=target_sentences).items():\n",
    "        output[key] = float(np.mean(vals))\n",
    "    \n",
    "    return output\n",
    "#     bert_metric.update(bert_metric, pred_senteces, target_sentences)\n",
    "#     return bert_metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kernel_prefixes(kernel : str) -> Set[str]:\n",
    "    prefixes = set()\n",
    "    one_line_kernel = kernel.replace(\"\\n\", \" \")\n",
    "    cuda_header_prefix_re = re.compile(\"__(host|global|device)__\")\n",
    "    \n",
    "    prefixes.update(cuda_header_prefix_re.findall(one_line_kernel))\n",
    "    return prefixes\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(sources : List[str], targets : List[str], preds : List[str], tokenizer, model : torch.nn.Module, used_ratio : float = 1.0):\n",
    "    assert used_ratio > 0 and used_ratio <= 1\n",
    "    assert len(targets) == len(preds) and len(targets) == len(sources)\n",
    "    \n",
    "    samples = list(zip(targets, preds))\n",
    "    np.random.shuffle(samples)\n",
    "    n = round(len(samples) * used_ratio)\n",
    "        \n",
    "    metrics_d = {\n",
    "        \"device\" : {},\n",
    "        \"host\" : {},\n",
    "        \"global\" : {}\n",
    "    }\n",
    "    \n",
    "    device_sentences = []\n",
    "    host_sentences = []\n",
    "    global_sentences = []\n",
    "    \n",
    "    pb = tqdm(zip(sources, targets, preds))\n",
    "    i = 0\n",
    "    for src, target, pred in pb:\n",
    "\n",
    "        found = False\n",
    "        \n",
    "        if found := (src.find(\"__device__\") != -1):\n",
    "            device_sentences.append((target, pred))\n",
    "        if found := (src.find(\"__host__\") != -1):\n",
    "            host_sentences.append((target, pred))\n",
    "        if found := (src.find(\"__global__\") != -1):\n",
    "            global_sentences.append((target, pred))\n",
    "        \n",
    "        if not found:\n",
    "            continue\n",
    "        \n",
    "        i += 1\n",
    "        if i >= n:\n",
    "            break\n",
    "    \n",
    "    if len(device_sentences) == 0:\n",
    "        print(\"WARNING: no device sample\")\n",
    "        # metrics_d[\"__device__\"] = {\"rouge\" : 0, \"bleu\" : 0, \"bert\" : 0}\n",
    "    else:\n",
    "        device_targets, device_preds = zip(*device_sentences)\n",
    "        metrics_d[\"device\"] = {\n",
    "            \"rouge\" : compute_rouge_score(device_targets, device_preds, tokenizer),\n",
    "            \"bleu\" : compute_bleu_score(device_targets, device_preds, tokenizer),\n",
    "            \"bert\" : compute_bert_score2(device_targets, device_preds, tokenizer, model)\n",
    "        }\n",
    "        print(f\"__device__ ({len(device_sentences)})\")\n",
    "        pprint(metrics_d[\"device\"])\n",
    "        \n",
    "    if len(host_sentences) == 0:\n",
    "        print(\"WARNING: no host samples\")\n",
    "        # metrics_d[\"__host__\"] = {\"rouge\" : 0, \"bleu\" : 0, \"bert\" : 0}\n",
    "    else:\n",
    "        host_targets, host_preds = zip(*host_sentences)\n",
    "        metrics_d[\"host\"] = {\n",
    "            \"rouge\" : compute_rouge_score(host_targets, host_preds, tokenizer),\n",
    "            \"bleu\" : compute_bleu_score(host_targets, host_preds, tokenizer),\n",
    "            \"bert\" : compute_bert_score2(host_targets, host_preds, tokenizer, model)\n",
    "        }\n",
    "        print(f\"__host__ ({len(host_sentences)})\")\n",
    "        pprint(metrics_d[\"host\"])\n",
    "        \n",
    "    if len(global_sentences) == 0:\n",
    "        print(\"WARNING: no global samples\")\n",
    "        # metrics_d[\"__global__\"] = {\"rouge\" : 0, \"bleu\" : 0, \"bert\" : 0}\n",
    "    else:\n",
    "        global_targets, global_preds = zip(*global_sentences)\n",
    "        metrics_d[\"global\"] = {\n",
    "            \"rouge\" : compute_rouge_score(global_targets, global_preds, tokenizer),\n",
    "            \"bleu\" : compute_bleu_score(global_targets, global_preds, tokenizer),\n",
    "            \"bert\" : compute_bert_score2(global_targets, global_preds, tokenizer, model)\n",
    "        }\n",
    "        print(f\"__global__ ({len(global_sentences)})\")\n",
    "        pprint(metrics_d[\"global\"])\n",
    "        \n",
    "    # Weights \n",
    "    total = len(device_sentences) + len(host_sentences) + len(global_sentences)\n",
    "    d_w = len(device_sentences) / total\n",
    "    h_w = len(host_sentences) / total\n",
    "    g_w = len(global_sentences) / total\n",
    "    \n",
    "    def calculate_total_metric(ds : List[dict], ws : List[float]):        \n",
    "        def calculate(ds : List[dict], ws : List[float], key : str):\n",
    "            r = 0\n",
    "            for d, w in zip(ds, ws):\n",
    "                if d is None:\n",
    "                    continue\n",
    "                \n",
    "                r += d.get(key, 0) * w\n",
    "            return r\n",
    "        \n",
    "        total_rouge = {}\n",
    "        \n",
    "        for d in ds:\n",
    "            if d is None:\n",
    "                continue\n",
    "            for key in d.keys():\n",
    "                if key in total_rouge:\n",
    "                    continue\n",
    "                \n",
    "                total_rouge[key] = calculate(ds, ws, key)\n",
    "        return total_rouge\n",
    "    \n",
    "    metrics_d[\"total\"] = {\"rouge\" : {}, \"bleu\" : -1, \"bert\" : {}}\n",
    "    metrics_d[\"total\"][\"rouge\"] = calculate_total_metric(\n",
    "        [metrics_d[\"device\"].get(\"rouge\"), metrics_d[\"host\"].get(\"rouge\"), metrics_d[\"global\"].get(\"rouge\")],\n",
    "        [d_w, h_w, g_w]\n",
    "    )\n",
    "    \n",
    "    metrics_d[\"total\"][\"bleu\"] = metrics_d[\"device\"].get(\"bleu\",0)*d_w + metrics_d[\"host\"].get(\"bleu\",0)*h_w + metrics_d[\"global\"].get(\"bleu\",0)*g_w\n",
    "\n",
    "    \n",
    "    metrics_d[\"total\"][\"bert\"] = calculate_total_metric(\n",
    "        [metrics_d[\"device\"].get(\"bert\"), metrics_d[\"host\"].get(\"bert\"), metrics_d[\"global\"].get(\"bert\")],\n",
    "        [d_w, h_w, g_w]\n",
    "    )\n",
    "    \n",
    "    print(\"total\")\n",
    "    pprint(metrics_d[\"total\"])\n",
    "    \n",
    "    # rouge = compute_rouge_score(targets, preds, tokenizer)\n",
    "    # pprint({\"rouge\" : rouge})\n",
    "\n",
    "    # bleu = compute_bleu_score(targets, preds, tokenizer)\n",
    "    # pprint({\"bleu\" : bleu})\n",
    "\n",
    "    # bert = compute_bert_score2(targets, preds, tokenizer, model)\n",
    "    # pprint({\"bert\" : bert})\n",
    "    \n",
    "    return metrics_d"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt2_model(model_name : str, model_path : str, tokenizer):\n",
    "\n",
    "    configuration = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_config(configuration).to(DEVICE)\n",
    "    # model.resize_token_embeddings(len(tokenizer))\n",
    "    model_dict = torch.load(model_path, map_location=\"cpu\")\n",
    "    model.load_state_dict(model_dict[\"model_dict\"])\n",
    "    \n",
    "    return model, model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilgpt2\"\n",
    "model_path = \"../../../models/gpt2/gpt2.evaluated.pt\"\n",
    "tokenizer_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=False, model_max_length=700, padding_side = \"left\", padding=True, truncation=True,)\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.add_special_tokens({\n",
    "    \"pad_token\" : \"<pad>\"\n",
    "})\n",
    "model, model_d = get_gpt2_model(model_name, model_path, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_ratio = 0.001\n",
    "metrics = compute_metrics(model_d.get(\"source_sentences\"), model_d.get(\"target_sentences\"), model_d.get(\"predic_sentences\"), tokenizer, model, used_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list = model_d.get(\"loss_list\")\n",
    "plt.figure()\n",
    "plt.title(\"GPT2 loss\")\n",
    "plt.ylabel(\"Cross-entropy loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.plot(loss_list)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Epochs: {len(loss_list)}\")\n",
    "print(f\"Max: {max(loss_list)}, epoch: {np.argmax(loss_list)}\")\n",
    "print(f\"Min: {min(loss_list)}, epoch: {np.argmin(loss_list)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgr_ls = 0\n",
    "tgr_cs = 0\n",
    "prd_ls = 0\n",
    "prd_cs = 0\n",
    "s = len(model_d.get(\"source_sentences\"))\n",
    "for tgr, prd in tqdm(zip(model_d.get(\"source_sentences\"), model_d.get(\"target_sentences\"))):\n",
    "    # tgr = tgr.replace(\";\", \";\\n\").replace(\"{\", \"{\\n\").replace(\"}\", \"}\\n\").replace(\"int\", \"\\nint\").replace(\"float\", \"\\nfloat\")\n",
    "    # prd = prd.replace(\";\", \";\\n\").replace(\"{\", \"{\\n\").replace(\"}\", \"}\\n\").replace(\"int\", \"\\nint\").replace(\"float\", \"\\nfloat\")\n",
    "    \n",
    "    tgr_ls += tgr.count(\"\\n\")\n",
    "    prd_ls += prd.count(\"\\n\")\n",
    "    \n",
    "    tgr_cs += len(tgr)\n",
    "    prd_cs += len(prd)\n",
    "    \n",
    "print(\"Average target lines per sample: {:.3f}\".format(tgr_ls/s))\n",
    "print(\"Average predic lines per sample: {:.3f}\".format(prd_ls/s))\n",
    "print(\"Average target sample size: {:.3f}\".format(tgr_cs/s))\n",
    "print(\"Average predic sample size: {:.3f}\".format(prd_cs/s))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random example from evaluation\n",
    "samples = list(zip(model_d.get(\"source_sentences\"), model_d.get(\"target_sentences\"), model_d.get(\"predic_sentences\")))\n",
    "np.random.shuffle(samples)\n",
    "src_l, tgr_l, prd_l = zip(*samples)\n",
    "for src, tgr, prd in zip(src_l, tgr_l, prd_l):\n",
    "    \n",
    "    if len(tgr) > 300:\n",
    "        continue\n",
    "    \n",
    "    print(\"SOURCE:\")\n",
    "    print(src)\n",
    "    print(\"\\nTARGET:\")\n",
    "    print(tgr)\n",
    "    print(\"\\nPREDICT:\")\n",
    "    print(prd)\n",
    "    print(\"#--------------------------#\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "// Kernel for dividing by two\n",
    "__device__ float divideByTwo(float v)\n",
    "\"\"\".strip()\n",
    "\n",
    "target = \"\"\"\n",
    "{\n",
    "    return v / 2;\n",
    "}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression \n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def scatter_hist(x, y, ax, ax_histx, ax_histy, title):\n",
    "    # no labels\n",
    "    ax_histx.tick_params(axis=\"x\", labelbottom=False)\n",
    "    ax_histy.tick_params(axis=\"y\", labelleft=False)\n",
    "\n",
    "    # the scatter plot:\n",
    "    ax.scatter(x, y)\n",
    "    \n",
    "    regressor = LinearRegression() \n",
    "    regressor.fit(np.array(x).reshape((len(x),1)), y)\n",
    "    p_y = regressor.predict(np.array(x).reshape((len(x),1)))\n",
    "    corr, _ = pearsonr(x, y)\n",
    "    ax.plot(x, p_y, linewidth=0.75, color=\"orange\")\n",
    "    ax.text(max(x)*0.7, max(p_y)*1.1, f\"Pear. corr={corr:.2f}\", weight='bold')\n",
    "\n",
    "    # now determine nice limits by hand:\n",
    "    binwidth = 0.25\n",
    "    \n",
    "    xmax = np.max(np.abs(x))\n",
    "    xlim = (int(xmax/binwidth) + 1) * binwidth\n",
    "    xbins = np.linspace(-xlim, xlim + binwidth, 50)\n",
    "    \n",
    "    ymax = np.max(np.abs(y))\n",
    "    ylim = (int(ymax/binwidth) + 1) * binwidth\n",
    "    ybins = np.linspace(-ylim, ylim + binwidth, 50)\n",
    "\n",
    "    ax_histx.hist(x, bins=xbins)\n",
    "    ax_histx.set_title(title)\n",
    "    ax_histy.hist(y, bins=ybins, orientation='horizontal')\n",
    "\n",
    "def plot_scatter_with_dist(x, y, title, xlabel, ylabel):\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Start with a square Figure.\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    gs = fig.add_gridspec(2, 2,  width_ratios=(4, 1), height_ratios=(1, 4),\n",
    "                        left=0.1, right=0.9, bottom=0.1, top=0.9,\n",
    "                        wspace=0.05, hspace=0.05)\n",
    "    # Create the Axes.\n",
    "    ax = fig.add_subplot(gs[1, 0])\n",
    "    ax_histx = fig.add_subplot(gs[0, 0], sharex=ax)\n",
    "    ax_histy = fig.add_subplot(gs[1, 1], sharey=ax)\n",
    "    \n",
    "    min_x = np.quantile(x, 0.05)\n",
    "    max_x = np.quantile(x, 0.95)\n",
    "    min_y = np.quantile(y, 0.05)\n",
    "    max_y = np.quantile(y, 0.95)\n",
    "    \n",
    "    x_mask = ((x > min_x) & (x < max_x))\n",
    "    x = x[x_mask]\n",
    "    y = y[x_mask]\n",
    "    y_mask = ((y > min_y) & (y < max_y))\n",
    "    x = x[y_mask]\n",
    "    y = y[y_mask]\n",
    "    \n",
    "    # Draw the scatter plot and marginals.\n",
    "    scatter_hist(x, y, ax, ax_histx, ax_histy, title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    \n",
    "    ax.set_xlim(min_x*0.9, max_x*1.1)\n",
    "    ax.set_ylim(min_y*0.9, max_y*1.1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_sample_len = []\n",
    "tgr_sample_len = []\n",
    "\n",
    "for prd, tgr in zip(model_d[\"predic_sentences\"], model_d[\"target_sentences\"]):\n",
    "    prd_sample_len.append(len(prd))\n",
    "    tgr_sample_len.append(len(tgr))\n",
    "\n",
    "plot_scatter_with_dist(prd_sample_len, tgr_sample_len, \"Prediction & target length\", \"Predic. length\", \"Target length\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze use of indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_c = 0\n",
    "global_f = 0\n",
    "other_c = 0\n",
    "other_f = 0\n",
    "\n",
    "for src, prd in tqdm(list(zip(model_d.get(\"source_sentences\"), model_d.get(\"predic_sentences\")))):\n",
    "    one_line_src = src.replace(\"\\n\", \" \")\n",
    "    is_global = one_line_src.find(\"__global__\") != -1\n",
    "    \n",
    "    prd_lines = prd.splitlines()\n",
    "    found = False\n",
    "    for line in prd_lines:\n",
    "        if line.find(\"threadIdx.\") != -1 and line.find(\"blockIdx.\") != -1 and line.find(\"blockDim.\") != -1:\n",
    "            if is_global:\n",
    "                global_c += 1\n",
    "            else:\n",
    "                other_f += 1\n",
    "            found = True\n",
    "            break\n",
    "        \n",
    "    if not found:\n",
    "        if is_global:\n",
    "            global_f += 1\n",
    "        else:\n",
    "            other_c += 1\n",
    "\n",
    "print(f\"global_c: {global_c} ({global_c/(global_c+global_f):.2%})\")\n",
    "print(f\"global_f: {global_f} ({global_f/(global_c+global_f):.2%})\")\n",
    "print(f\"others_c: {other_c} ({other_c/(other_c+other_f):.2%})\")\n",
    "print(f\"others_f: {other_f} ({other_f/(other_c+other_f):.2%})\")\n",
    "\n",
    "total = global_c + global_f + other_c + other_f\n",
    "correct = global_c + other_c\n",
    "incorrect = global_f + other_f\n",
    "print(f\"Correct: {correct} ({correct/total:.2%})\")\n",
    "print(f\"Incorrect: {incorrect}, ({incorrect/total:.2%})\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use of local memory analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "t_p = 0\n",
    "t_n = 0\n",
    "f_p = 0\n",
    "f_n = 0\n",
    "\n",
    "t = []\n",
    "p = []\n",
    "\n",
    "for tgr, prd in tqdm(list(zip(model_d.get(\"target_sentences\"), model_d.get(\"predic_sentences\")))):\n",
    "    tgr_l_m_use = tgr.find(\"__shared__\") != -1 or tgr.find(\"__constant__\") != -1\n",
    "    prd_l_m_use = prd.find(\"__shared__\") != -1 or prd.find(\"__constant__\") != -1\n",
    "    \n",
    "    if tgr_l_m_use:\n",
    "        if prd_l_m_use:\n",
    "            t_p += 1\n",
    "        else:\n",
    "            f_n += 1\n",
    "    else:\n",
    "        if prd_l_m_use:\n",
    "            f_p += 1\n",
    "        else:\n",
    "            t_n += 1\n",
    "            \n",
    "    t.append(tgr_l_m_use)\n",
    "    p.append(prd_l_m_use)\n",
    "    \n",
    "total = t_p + t_n + f_p + f_n\n",
    "print(f\"True positive: {t_p}\\t({t_p/total:.2%})\")\n",
    "print(f\"True negative: {t_n}\\t({t_n/total:.2%})\")\n",
    "print(f\"False positive: {f_p}\\t({f_p/total:.2%})\")\n",
    "print(f\"False negative: {f_n}\\t({f_n/total:.2%})\")\n",
    "\n",
    "precision = precision_score(t, p)\n",
    "recall = recall_score(t, p)\n",
    "f1 = f1_score(t, p)\n",
    "\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1: {f1:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diplomka",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
