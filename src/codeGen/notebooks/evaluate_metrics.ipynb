{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re, json, csv\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from pymongo import MongoClient\n",
    "from tqdm.notebook import tqdm\n",
    "from pprint import PrettyPrinter\n",
    "from typing import Union, Iterable\n",
    "from bson.objectid import ObjectId\n",
    "from torchmetrics.text.bert import BERTScore\n",
    "from torchmetrics.text.rouge import ROUGEScore\n",
    "from torchmetrics.text.bleu import BLEUScore\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoModelForCausalLM, AutoConfig, AutoTokenizer, pipeline, set_seed\n",
    "from torchmetrics.functional import pairwise_cosine_similarity\n",
    "from torchmetrics.functional.text.bert import _get_precision_recall_f1\n",
    "from functools import partial\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import chain\n",
    "from math import log\n",
    "from multiprocessing import Pool\n",
    "from typing import List\n",
    "from pymongo import MongoClient\n",
    "from typing import Set\n",
    "from collections import OrderedDict\n",
    "\n",
    "pprint = PrettyPrinter().pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = MongoClient(\"mongodb://localhost:27017\")[\"cuda_snippets\"]\n",
    "train_db = db[\"train\"]\n",
    "validation_db = db[\"validation\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rouge_score(target_sentences : Iterable[str], pred_senteces : Iterable[str], tokenizer):\n",
    "    tokenizer_fn = lambda x: tokenizer.convert_ids_to_tokens(tokenizer.encode(x))\n",
    "    rouge_metric = ROUGEScore(tokenizer=tokenizer_fn)\n",
    "    # rouge_metric.update(pred_senteces, target_sentences)\n",
    "    return rouge_metric(pred_senteces, target_sentences)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bleu_score(target_sentences : Iterable[str], pred_senteces : Iterable[str], tokenizer):\n",
    "    bleu_metric = BLEUScore(tokenizer=tokenizer)\n",
    "    bleu_metric.update(pred_senteces, [[s] for s in target_sentences])\n",
    "    return bleu_metric.compute()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate BERT score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(a, tokenizer=None):\n",
    "    if tokenizer is not None:\n",
    "        a = tokenizer.encode(a, max_length=512, truncation=True, add_special_tokens=False)\n",
    "    return set(a)\n",
    "\n",
    "def get_idf_dict(arr, tokenizer, nthreads=4):\n",
    "    idf_count = Counter()\n",
    "    num_docs = len(arr)\n",
    "\n",
    "    process_partial = partial(process, tokenizer=tokenizer)\n",
    "\n",
    "    if nthreads > 0:\n",
    "        with Pool(nthreads) as p:\n",
    "            idf_count.update(chain.from_iterable(p.map(process_partial, arr)))\n",
    "    else:\n",
    "        idf_count.update(chain.from_iterable(map(process_partial, arr)))\n",
    "\n",
    "    idf_dict = defaultdict(lambda: log((num_docs + 1) / (1)))\n",
    "    idf_dict.update(\n",
    "        {idx: log((num_docs + 1) / (c + 1)) for (idx, c) in idf_count.items()}\n",
    "    )\n",
    "    return idf_dict\n",
    "\n",
    "class TextDataset:\n",
    "    def __init__(self, target_sentences, pred_sentences) -> None:\n",
    "        self.target_sentences = target_sentences\n",
    "        self.pred_sentences = pred_sentences\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.target_sentences)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.target_sentences[i], self.pred_sentences[i]\n",
    "    \n",
    "class CollateFn:\n",
    "    def __init__(self, tokenizer) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __call__(self, samples : list):\n",
    "        targets, preds = zip(*samples)\n",
    "        \n",
    "        targets_pt = self.tokenizer(targets, max_length=512, truncation=True, return_tensors=\"pt\", add_special_tokens=False, padding=True)\n",
    "        preds_pt = self.tokenizer(preds, max_length=512, truncation=True, return_tensors=\"pt\", add_special_tokens=False, padding=True)\n",
    "\n",
    "        return (targets_pt, targets), (preds_pt, preds)\n",
    "\n",
    "def compute_bert_score(target_sentences : Iterable[str], pred_sentences : Iterable[str], tokenizer, embedd_layer : torch.nn.Embedding, idf_scores : dict = None, batch_size=64):\n",
    "\n",
    "    if idf_scores is None:\n",
    "        idf_scores = get_idf_dict(target_sentences, tokenizer)\n",
    "    precs = []\n",
    "    recals = []\n",
    "    f1s = []\n",
    "    \n",
    "    dataset = TextDataset(target_sentences, pred_sentences)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=CollateFn(tokenizer))\n",
    "    \n",
    "    for i, ((target_pt, target_str), (pred_pt, pred_str)) in enumerate(tqdm(dataloader)):\n",
    "\n",
    "        pred_embedd = embedd_layer(pred_pt[\"input_ids\"]).view((pred_pt[\"input_ids\"].size(0),1,pred_pt[\"input_ids\"].size(1), -1))\n",
    "        target_embedd = embedd_layer(target_pt[\"input_ids\"]).view((target_pt[\"input_ids\"].size(0),1,target_pt[\"input_ids\"].size(1), -1))\n",
    "        \n",
    "        pred_idfs = [tokenizer.encode(sentence, max_length=512, add_special_tokens=False, truncation=True, return_tensors=\"pt\")[0] for sentence in pred_str]\n",
    "        pred_idfs = [torch.Tensor([idf_scores.get(token, 0) for token in sentence]) for sentence in pred_idfs]\n",
    "        pred_idfs = torch.nn.utils.rnn.pad_sequence(pred_idfs, batch_first=True, padding_value=0)\n",
    "        \n",
    "        target_idfs = [tokenizer.encode(sentence, max_length=512, add_special_tokens=False, truncation=True, return_tensors=\"pt\")[0] for sentence in target_str]\n",
    "        target_idfs = [torch.Tensor([idf_scores.get(token, 0) for token in sentence]) for sentence in target_idfs]\n",
    "        target_idfs = torch.nn.utils.rnn.pad_sequence(target_idfs, batch_first=True, padding_value=0)\n",
    "\n",
    "        # pred_idfs = torch.Tensor(\n",
    "        #     [[idf_scores.get(token, 0) for token in sentence] for sentence in pred_str]\n",
    "        # )\n",
    "        \n",
    "        # target_idfs = torch.Tensor(\n",
    "        #     [[idf_scores.get(token, 0) for token in sentence] for sentence in target_str]\n",
    "        # )\n",
    "        \n",
    "        # cos_sims = pairwise_cosine_similarity(target_embedd, pred_embedd)\n",
    "        \n",
    "        # max_sims = torch.max(cos_sims, dim=1)[0]\n",
    "        # sentence_scores.append(sum(max_sims*target_idf)/sum(target_idf))\n",
    "        \n",
    "        prec, res, f1 = _get_precision_recall_f1(pred_embedd, target_embedd, pred_idfs, target_idfs)\n",
    "        precs.append(torch.mean(prec))\n",
    "        recals.append(torch.mean(res))\n",
    "        f1s.append(torch.mean(f1))\n",
    "        \n",
    "    return  sum(precs) / len(precs), \\\n",
    "            sum(recals) / len(recals), \\\n",
    "            sum(f1s) / len(f1s)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(self, preds: List[str], targets: List[str]) -> None:\n",
    "        \"\"\"Store predictions/references for computing BERT scores.\n",
    "\n",
    "        It is necessary to store sentences in a tokenized form to ensure the DDP mode working.\n",
    "        \"\"\"\n",
    "        \n",
    "        preds_tokenized_data = self.tokenizer(preds, max_length=1024, truncation=True, return_tensors=\"pt\", padding=True)\n",
    "        preds_dict = {\"input_ids\": preds_tokenized_data[\"input_ids\"], \"attention_mask\": preds_tokenized_data[\"attention_mask\"]}\n",
    "        target_tokenized_data = self.tokenizer(targets, max_length=1024, truncation=True, return_tensors=\"pt\", padding=True)\n",
    "        target_dict = {\"input_ids\": target_tokenized_data[\"input_ids\"], \"attention_mask\": target_tokenized_data[\"attention_mask\"]}\n",
    "\n",
    "        self.preds_input_ids.append(preds_dict[\"input_ids\"])\n",
    "        self.preds_attention_mask.append(preds_dict[\"attention_mask\"])\n",
    "        self.target_input_ids.append(target_dict[\"input_ids\"])\n",
    "        self.target_attention_mask.append(target_dict[\"attention_mask\"])\n",
    "        \n",
    "def compute_bert_score2(target_sentences : Iterable[str], pred_senteces : Iterable[str], tokenizer, model : torch.nn.Module, *args, **kwargs):\n",
    "    BERTScore.update = update\n",
    "    user_forward_fn = lambda model, d: model.get_input_embeddings()(d[\"input_ids\"])\n",
    "    bert_metric = BERTScore(model=model, user_tokenizer=tokenizer, user_forward_fn=user_forward_fn, device=DEVICE, verbose=True, num_layers=0, max_length=1024)\n",
    "    # bert_metric.update = update\n",
    "    output = {}\n",
    "    for key, vals in bert_metric(preds=pred_senteces, targets=target_sentences).items():\n",
    "        output[key] = float(np.mean(vals))\n",
    "    \n",
    "    return output\n",
    "#     bert_metric.update(bert_metric, pred_senteces, target_sentences)\n",
    "#     return bert_metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kernel_prefixes(kernel : str) -> Set[str]:\n",
    "    prefixes = set()\n",
    "    one_line_kernel = kernel.replace(\"\\n\", \" \")\n",
    "    cuda_header_prefix_re = re.compile(\"__(host|global|device)__\")\n",
    "    \n",
    "    prefixes.update(cuda_header_prefix_re.findall(one_line_kernel))\n",
    "    return prefixes\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(sources : List[str], targets : List[str], preds : List[str], tokenizer, model : torch.nn.Module, used_ratio : float = 1.0):\n",
    "    assert used_ratio > 0 and used_ratio <= 1\n",
    "    assert len(targets) == len(preds)\n",
    "    \n",
    "    samples = list(zip(targets, preds))\n",
    "    np.random.shuffle(samples)\n",
    "    n = round(len(samples) * used_ratio)\n",
    "        \n",
    "    metrics_d = {\n",
    "        \"device\" : {},\n",
    "        \"host\" : {},\n",
    "        \"global\" : {}\n",
    "    }\n",
    "    \n",
    "    device_sentences = []\n",
    "    host_sentences = []\n",
    "    global_sentences = []\n",
    "    \n",
    "    pb = tqdm(zip(sources, targets, preds))\n",
    "    i = 0\n",
    "    for src, target, pred in pb:\n",
    "        \n",
    "        found = False\n",
    "        \n",
    "        if found := (src.find(\"__device__\") != -1):\n",
    "            device_sentences.append((target, pred))\n",
    "        if found := (src.find(\"__host__\") != -1):\n",
    "            host_sentences.append((target, pred))\n",
    "        if found := (src.find(\"__global__\") != -1):\n",
    "            global_sentences.append((target, pred))\n",
    "        \n",
    "        if not found:\n",
    "            continue\n",
    "        \n",
    "        i += 1\n",
    "        if i >= n:\n",
    "            break\n",
    "    \n",
    "    if len(device_sentences) == 0:\n",
    "        print(\"WARNING: no device sample\")\n",
    "        # metrics_d[\"__device__\"] = {\"rouge\" : 0, \"bleu\" : 0, \"bert\" : 0}\n",
    "    else:\n",
    "        device_targets, device_preds = zip(*device_sentences)\n",
    "        metrics_d[\"device\"] = {\n",
    "            \"rouge\" : compute_rouge_score(device_targets, device_preds, tokenizer),\n",
    "            \"bleu\" : compute_bleu_score(device_targets, device_preds, tokenizer),\n",
    "            \"bert\" : compute_bert_score2(device_targets, device_preds, tokenizer, model)\n",
    "        }\n",
    "        print(f\"__device__ ({len(device_sentences)})\")\n",
    "        pprint(metrics_d[\"device\"])\n",
    "        \n",
    "    if len(host_sentences) == 0:\n",
    "        print(\"WARNING: no host samples\")\n",
    "        # metrics_d[\"__host__\"] = {\"rouge\" : 0, \"bleu\" : 0, \"bert\" : 0}\n",
    "    else:\n",
    "        host_targets, host_preds = zip(*host_sentences)\n",
    "        metrics_d[\"host\"] = {\n",
    "            \"rouge\" : compute_rouge_score(host_targets, host_preds, tokenizer),\n",
    "            \"bleu\" : compute_bleu_score(host_targets, host_preds, tokenizer),\n",
    "            \"bert\" : compute_bert_score2(host_targets, host_preds, tokenizer, model)\n",
    "        }\n",
    "        print(f\"__host__ ({len(host_sentences)})\")\n",
    "        pprint(metrics_d[\"host\"])\n",
    "        \n",
    "    if len(global_sentences) == 0:\n",
    "        print(\"WARNING: no global samples\")\n",
    "        # metrics_d[\"__global__\"] = {\"rouge\" : 0, \"bleu\" : 0, \"bert\" : 0}\n",
    "    else:\n",
    "        global_targets, global_preds = zip(*global_sentences)\n",
    "        metrics_d[\"global\"] = {\n",
    "            \"rouge\" : compute_rouge_score(global_targets, global_preds, tokenizer),\n",
    "            \"bleu\" : compute_bleu_score(global_targets, global_preds, tokenizer),\n",
    "            \"bert\" : compute_bert_score2(global_targets, global_preds, tokenizer, model)\n",
    "        }\n",
    "        print(f\"__global__ ({len(global_sentences)})\")\n",
    "        pprint(metrics_d[\"global\"])\n",
    "        \n",
    "    # Weights \n",
    "    total = len(device_sentences) + len(host_sentences) + len(global_sentences)\n",
    "    d_w = len(device_sentences) / total\n",
    "    h_w = len(host_sentences) / total\n",
    "    g_w = len(global_sentences) / total\n",
    "    \n",
    "    def calculate_total_metric(ds : List[dict], ws : List[float]):        \n",
    "        def calculate(ds : List[dict], ws : List[float], key : str):\n",
    "            r = 0\n",
    "            for d, w in zip(ds, ws):\n",
    "                if d is None:\n",
    "                    continue\n",
    "                \n",
    "                r += d.get(key, 0) * w\n",
    "            return r\n",
    "        \n",
    "        total_rouge = {}\n",
    "        \n",
    "        for d in ds:\n",
    "            if d is None:\n",
    "                continue\n",
    "            for key in d.keys():\n",
    "                if key in total_rouge:\n",
    "                    continue\n",
    "                \n",
    "                total_rouge[key] = calculate(ds, ws, key)\n",
    "        return total_rouge\n",
    "    \n",
    "    metrics_d[\"total\"] = {\"rouge\" : {}, \"bleu\" : -1, \"bert\" : {}}\n",
    "    metrics_d[\"total\"][\"rouge\"] = calculate_total_metric(\n",
    "        [metrics_d[\"device\"].get(\"rouge\"), metrics_d[\"host\"].get(\"rouge\"), metrics_d[\"global\"].get(\"rouge\")],\n",
    "        [d_w, h_w, g_w]\n",
    "    )\n",
    "    \n",
    "    metrics_d[\"total\"][\"bleu\"] = metrics_d[\"device\"].get(\"bleu\",0)*d_w + metrics_d[\"host\"].get(\"bleu\",0)*h_w + metrics_d[\"global\"].get(\"bleu\",0)*g_w\n",
    "\n",
    "    \n",
    "    metrics_d[\"total\"][\"bert\"] = calculate_total_metric(\n",
    "        [metrics_d[\"device\"].get(\"bert\"), metrics_d[\"host\"].get(\"bert\"), metrics_d[\"global\"].get(\"bert\")],\n",
    "        [d_w, h_w, g_w]\n",
    "    )\n",
    "    \n",
    "    print(\"total\")\n",
    "    pprint(metrics_d[\"total\"])\n",
    "    \n",
    "    # rouge = compute_rouge_score(targets, preds, tokenizer)\n",
    "    # pprint({\"rouge\" : rouge})\n",
    "\n",
    "    # bleu = compute_bleu_score(targets, preds, tokenizer)\n",
    "    # pprint({\"bleu\" : bleu})\n",
    "\n",
    "    # bert = compute_bert_score2(targets, preds, tokenizer, model)\n",
    "    # pprint({\"bert\" : bert})\n",
    "    \n",
    "    return metrics_d"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CodeGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_codegen_model(model_name : str, model_path : str, tokenizer):\n",
    "\n",
    "    configuration = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_config(configuration).to(DEVICE)\n",
    "    model.resize_token_embeddings(51200)\n",
    "    model_dict = torch.load(model_path, map_location=DEVICE)\n",
    "    # model_state_dict = OrderedDict()\n",
    "    # for key, val in model_dict[\"model_dict\"].items():\n",
    "    #     model_state_dict[\".\".join(key.split(\".\")[1:])] = val\n",
    "    # model.load_state_dict(model_state_dict)\n",
    "    model.load_state_dict(model_dict[\"model_dict\"])\n",
    "    \n",
    "    return model, model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codegen_model_name = \"Salesforce/codegen-350M-multi\"\n",
    "codegen_model_path = \"/home/xsaman02/Project/models/codeGen/codegen.evaluated.pt\"\n",
    "codegen_tokenizer_name = \"Salesforce/codegen-350M-multi\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(codegen_model_name, use_fast=False, model_max_length=700, add_bos_token=True)\n",
    "tokenizer.add_special_tokens({\n",
    "    \"pad_token\" : \"</s>\"\n",
    "})\n",
    "model, model_d = get_codegen_model(codegen_model_name, codegen_model_path, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "976083fd45eb42d3b96dfc9739140105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (906 > 700). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e2707e4e9d54853afb5df86506a118b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e60822fce78420a880a5df2c0f241fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f536e0d5af0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/xsaman02/miniconda3/envs/diplomka/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/xsaman02/miniconda3/envs/diplomka/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/xsaman02/miniconda3/envs/diplomka/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f536e0d5af0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/xsaman02/miniconda3/envs/diplomka/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/xsaman02/miniconda3/envs/diplomka/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/xsaman02/miniconda3/envs/diplomka/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__device__ (2289)\n",
      "{'bert': {'f1': 0.5419530822309984,\n",
      "          'precision': 0.533098743348832,\n",
      "          'recall': 0.566545169177197},\n",
      " 'bleu': tensor(0.3015),\n",
      " 'rouge': {'rouge1_fmeasure': tensor(0.2923),\n",
      "           'rouge1_precision': tensor(0.2223),\n",
      "           'rouge1_recall': tensor(1.),\n",
      "           'rouge2_fmeasure': tensor(0.1079),\n",
      "           'rouge2_precision': tensor(0.1079),\n",
      "           'rouge2_recall': tensor(0.1079),\n",
      "           'rougeL_fmeasure': tensor(0.2923),\n",
      "           'rougeL_precision': tensor(0.2223),\n",
      "           'rougeL_recall': tensor(1.),\n",
      "           'rougeLsum_fmeasure': tensor(0.2918),\n",
      "           'rougeLsum_precision': tensor(0.2220),\n",
      "           'rougeLsum_recall': tensor(1.)}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "458a1281a19848a48f64dde94b9f1aa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7001f3edb124549ac40a0551222648d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__host__ (1182)\n",
      "{'bert': {'f1': 0.5948891153593757,\n",
      "          'precision': 0.5704177752414532,\n",
      "          'recall': 0.6423766957170467},\n",
      " 'bleu': tensor(0.1369),\n",
      " 'rouge': {'rouge1_fmeasure': tensor(0.4302),\n",
      "           'rouge1_precision': tensor(0.3492),\n",
      "           'rouge1_recall': tensor(1.),\n",
      "           'rouge2_fmeasure': tensor(0.2064),\n",
      "           'rouge2_precision': tensor(0.2064),\n",
      "           'rouge2_recall': tensor(0.2064),\n",
      "           'rougeL_fmeasure': tensor(0.4302),\n",
      "           'rougeL_precision': tensor(0.3492),\n",
      "           'rougeL_recall': tensor(1.),\n",
      "           'rougeLsum_fmeasure': tensor(0.4297),\n",
      "           'rougeLsum_precision': tensor(0.3489),\n",
      "           'rougeLsum_recall': tensor(1.)}}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m used_ratio \u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m\n\u001b[0;32m----> 2\u001b[0m metrics \u001b[39m=\u001b[39m compute_metrics(model_d\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mtarget_sentences\u001b[39;49m\u001b[39m\"\u001b[39;49m), model_d\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mpredic_sentences\u001b[39;49m\u001b[39m\"\u001b[39;49m), tokenizer, model, used_ratio)\n",
      "Cell \u001b[0;32mIn[9], line 75\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[0;34m(targets, preds, tokenizer, model, used_ratio)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[39m# metrics_d[\"__global__\"] = {\"rouge\" : 0, \"bleu\" : 0, \"bert\" : 0}\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     global_targets, global_preds \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mglobal_sentences)\n\u001b[1;32m     74\u001b[0m     metrics_d[\u001b[39m\"\u001b[39m\u001b[39mglobal\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m {\n\u001b[0;32m---> 75\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mrouge\u001b[39m\u001b[39m\"\u001b[39m : compute_rouge_score(global_targets, global_preds, tokenizer),\n\u001b[1;32m     76\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbleu\u001b[39m\u001b[39m\"\u001b[39m : compute_bleu_score(global_targets, global_preds, tokenizer),\n\u001b[1;32m     77\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbert\u001b[39m\u001b[39m\"\u001b[39m : compute_bert_score2(global_targets, global_preds, tokenizer, model)\n\u001b[1;32m     78\u001b[0m     }\n\u001b[1;32m     79\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m__global__ (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(global_sentences)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     80\u001b[0m     pprint(metrics_d[\u001b[39m\"\u001b[39m\u001b[39mglobal\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m, in \u001b[0;36mcompute_rouge_score\u001b[0;34m(target_sentences, pred_senteces, tokenizer)\u001b[0m\n\u001b[1;32m      3\u001b[0m rouge_metric \u001b[39m=\u001b[39m ROUGEScore(tokenizer\u001b[39m=\u001b[39mtokenizer_fn)\n\u001b[1;32m      4\u001b[0m \u001b[39m# rouge_metric.update(pred_senteces, target_sentences)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[39mreturn\u001b[39;00m rouge_metric(pred_senteces, target_sentences)\n",
      "File \u001b[0;32m~/miniconda3/envs/diplomka/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/diplomka/lib/python3.8/site-packages/torchmetrics/metric.py:234\u001b[0m, in \u001b[0;36mMetric.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[39mraise\u001b[39;00m TorchMetricsUserError(\n\u001b[1;32m    229\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe Metric shouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be synced when performing ``forward``. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    230\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mHINT: Did you forget to call ``unsync`` ?.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    231\u001b[0m     )\n\u001b[1;32m    233\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfull_state_update \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfull_state_update \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdist_sync_on_step:\n\u001b[0;32m--> 234\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_cache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_full_state_update(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    235\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    236\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_cache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_reduce_state_update(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/diplomka/lib/python3.8/site-packages/torchmetrics/metric.py:247\u001b[0m, in \u001b[0;36mMetric._forward_full_state_update\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"forward computation using two calls to `update` to calculate the metric value on the current batch and\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[39maccumulate global state.\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \n\u001b[1;32m    244\u001b[0m \u001b[39mDoing this secures that metrics that need access to the full metric state during `update` works as expected.\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[39m# global accumulation\u001b[39;00m\n\u001b[0;32m--> 247\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    248\u001b[0m _update_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_count\n\u001b[1;32m    250\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_to_sync \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdist_sync_on_step\n",
      "File \u001b[0;32m~/miniconda3/envs/diplomka/lib/python3.8/site-packages/torchmetrics/metric.py:390\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_grad):\n\u001b[1;32m    389\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 390\u001b[0m         update(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    391\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    392\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mExpected all tensors to be on\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(err):\n",
      "File \u001b[0;32m~/miniconda3/envs/diplomka/lib/python3.8/site-packages/torchmetrics/text/rouge.py:150\u001b[0m, in \u001b[0;36mROUGEScore.update\u001b[0;34m(self, preds, target)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(target, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    148\u001b[0m     target \u001b[39m=\u001b[39m [[target]]\n\u001b[0;32m--> 150\u001b[0m output: Dict[Union[\u001b[39mint\u001b[39m, \u001b[39mstr\u001b[39m], List[Dict[\u001b[39mstr\u001b[39m, Tensor]]] \u001b[39m=\u001b[39m _rouge_score_update(\n\u001b[1;32m    151\u001b[0m     preds,\n\u001b[1;32m    152\u001b[0m     target,\n\u001b[1;32m    153\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrouge_keys_values,\n\u001b[1;32m    154\u001b[0m     stemmer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstemmer,\n\u001b[1;32m    155\u001b[0m     normalizer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalizer,\n\u001b[1;32m    156\u001b[0m     tokenizer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer,\n\u001b[1;32m    157\u001b[0m     accumulate\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccumulate,\n\u001b[1;32m    158\u001b[0m )\n\u001b[1;32m    159\u001b[0m \u001b[39mfor\u001b[39;00m rouge_key, metrics \u001b[39min\u001b[39;00m output\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    160\u001b[0m     \u001b[39mfor\u001b[39;00m metric \u001b[39min\u001b[39;00m metrics:\n",
      "File \u001b[0;32m~/miniconda3/envs/diplomka/lib/python3.8/site-packages/torchmetrics/functional/text/rouge.py:358\u001b[0m, in \u001b[0;36m_rouge_score_update\u001b[0;34m(preds, target, rouge_keys_values, accumulate, stemmer, normalizer, tokenizer)\u001b[0m\n\u001b[1;32m    356\u001b[0m     score \u001b[39m=\u001b[39m _rouge_l_score(pred, tgt)\n\u001b[1;32m    357\u001b[0m \u001b[39melif\u001b[39;00m rouge_key \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mLsum\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 358\u001b[0m     score \u001b[39m=\u001b[39m _rouge_lsum_score(pred_lsum, target_lsum)\n\u001b[1;32m    359\u001b[0m result_inner[rouge_key] \u001b[39m=\u001b[39m score\n\u001b[1;32m    360\u001b[0m result_avg[rouge_key]\u001b[39m.\u001b[39mappend(score)\n",
      "File \u001b[0;32m~/miniconda3/envs/diplomka/lib/python3.8/site-packages/torchmetrics/functional/text/rouge.py:269\u001b[0m, in \u001b[0;36m_rouge_lsum_score\u001b[0;34m(pred, target)\u001b[0m\n\u001b[1;32m    267\u001b[0m hits \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    268\u001b[0m \u001b[39mfor\u001b[39;00m tgt \u001b[39min\u001b[39;00m target:\n\u001b[0;32m--> 269\u001b[0m     lcs \u001b[39m=\u001b[39m _union_lcs(pred, tgt)\n\u001b[1;32m    270\u001b[0m     \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m lcs:\n\u001b[1;32m    271\u001b[0m         \u001b[39mif\u001b[39;00m pred_tokens_count[token] \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m target_tokens_count[token] \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/diplomka/lib/python3.8/site-packages/torchmetrics/functional/text/rouge.py:157\u001b[0m, in \u001b[0;36m_union_lcs\u001b[0;34m(pred_tokens_list, target_tokens)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Find union LCS given a list of LCS.\"\"\"\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msorted\u001b[39m(\u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m()\u001b[39m.\u001b[39munion(\u001b[39m*\u001b[39mlcs_tables)))\n\u001b[0;32m--> 157\u001b[0m lcs_tables \u001b[39m=\u001b[39m [lcs_ind(pred_tokens, target_tokens) \u001b[39mfor\u001b[39;00m pred_tokens \u001b[39min\u001b[39;00m pred_tokens_list]\n\u001b[1;32m    158\u001b[0m union_lcs \u001b[39m=\u001b[39m [target_tokens[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m find_union(lcs_tables)]\n\u001b[1;32m    159\u001b[0m \u001b[39mreturn\u001b[39;00m union_lcs\n",
      "File \u001b[0;32m~/miniconda3/envs/diplomka/lib/python3.8/site-packages/torchmetrics/functional/text/rouge.py:157\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Find union LCS given a list of LCS.\"\"\"\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msorted\u001b[39m(\u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m()\u001b[39m.\u001b[39munion(\u001b[39m*\u001b[39mlcs_tables)))\n\u001b[0;32m--> 157\u001b[0m lcs_tables \u001b[39m=\u001b[39m [lcs_ind(pred_tokens, target_tokens) \u001b[39mfor\u001b[39;00m pred_tokens \u001b[39min\u001b[39;00m pred_tokens_list]\n\u001b[1;32m    158\u001b[0m union_lcs \u001b[39m=\u001b[39m [target_tokens[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m find_union(lcs_tables)]\n\u001b[1;32m    159\u001b[0m \u001b[39mreturn\u001b[39;00m union_lcs\n",
      "File \u001b[0;32m~/miniconda3/envs/diplomka/lib/python3.8/site-packages/torchmetrics/functional/text/rouge.py:149\u001b[0m, in \u001b[0;36m_union_lcs.<locals>.lcs_ind\u001b[0;34m(pred_tokens, target_tokens)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlcs_ind\u001b[39m(pred_tokens: Sequence[\u001b[39mstr\u001b[39m], target_tokens: Sequence[\u001b[39mstr\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Sequence[\u001b[39mint\u001b[39m]:\n\u001b[1;32m    148\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Returns one of the longest of longest common subsequence via backtracked lcs table.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m     lcs_table: Sequence[Sequence[\u001b[39mint\u001b[39m]] \u001b[39m=\u001b[39m _lcs(pred_tokens, target_tokens, return_full_table\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m    150\u001b[0m     backtracked_lcs_table \u001b[39m=\u001b[39m _backtracked_lcs(lcs_table, pred_tokens, target_tokens)\n\u001b[1;32m    151\u001b[0m     \u001b[39mreturn\u001b[39;00m backtracked_lcs_table\n",
      "File \u001b[0;32m~/miniconda3/envs/diplomka/lib/python3.8/site-packages/torchmetrics/functional/text/rouge.py:106\u001b[0m, in \u001b[0;36m_lcs\u001b[0;34m(pred_tokens, target_tokens, return_full_table)\u001b[0m\n\u001b[1;32m    104\u001b[0m             lcs[i][j] \u001b[39m=\u001b[39m lcs[i \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m][j \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    105\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 106\u001b[0m             lcs[i][j] \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(lcs[i \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m][j], lcs[i][j \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m])\n\u001b[1;32m    107\u001b[0m \u001b[39mif\u001b[39;00m return_full_table:\n\u001b[1;32m    108\u001b[0m     \u001b[39mreturn\u001b[39;00m lcs\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "used_ratio = 0.1\n",
    "metrics = compute_metrics(model_d.get(\"target_sentences\"), model_d.get(\"predic_sentences\"), tokenizer, model, used_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAABNKklEQVR4nO3deXxU1f3/8fdMlslC9pANAgn7HjZFQKsIiOACiqLWr0BRUQEV0aq0KvirLWorbqVQN7C11q2KrQsKKCC7yo6ALAECJAFC9j0z9/dHkoFIEjIwk0lmXs/HYx7J3Hvm5nO55pG3555zj8kwDEMAAAAewuzuAgAAAJyJcAMAADwK4QYAAHgUwg0AAPAohBsAAOBRCDcAAMCjEG4AAIBHIdwAAACPQrgBAAAehXADwOWSkpI0ceJEd5fhVAcPHpTJZNKiRYvcXQqAXyDcAF5s//79uueee9SuXTsFBAQoNDRUgwcP1ssvv6zi4mJ3l2dXWlqqV199VZdeeqkiIiLk7++vhIQEXX/99fr3v/8tq9Xq7hIBNCG+7i4AgHt8/vnnuvnmm2WxWDR+/Hj16NFDZWVlWr16tX77299q586deu2119xdpk6cOKGRI0fqxx9/1IgRI/TEE08oMjJSGRkZWrZsmX79619r3759evLJJ91dKoAmgnADeKHU1FTdeuutatu2rb755hvFx8fb902dOlX79u3T559/7sYKT7vjjju0efNm/ec//9GNN95YY9/MmTP1ww8/aM+ePW6qDkBTxG0pwAs9//zzKigo0Jtvvlkj2FTr0KGDHnzwQfv7iooK/eEPf1D79u1lsViUlJSk3/3udyotLa3xOcMw9Mwzz6h169YKCgrSkCFDtHPnzlpryMnJ0fTp05WYmCiLxaIOHTroueeek81ms7dZt26dvvrqK02ePPmsYFOtf//+uv3222tsKy0t1axZs9ShQwdZLBYlJibq0UcfPatek8mkadOmafHixerRo4csFou6d++uJUuW1P8PWI9vvvlGl112mYKDgxUeHq7Ro0dr165dNdrk5+dr+vTpSkpKksViUUxMjIYPH65NmzbZ2+zdu1djx45VXFycAgIC1Lp1a916663Kzc0979oAb0HPDeCF/ve//6ldu3YaNGhQg9rfddddevvtt3XTTTfp4Ycf1oYNGzRnzhzt2rVLn3zyib3dU089pWeeeUajRo3SqFGjtGnTJl111VUqKyurcbyioiJdfvnlOnr0qO655x61adNGa9eu1cyZM5Wenq6XXnrJXqck/d///V+Dz81ms+n666/X6tWrNXnyZHXt2lXbt2/Xiy++qJ9//lmLFy+u0X716tX6+OOPNWXKFIWEhOiVV17R2LFjdfjwYUVFRTX450rSsmXLNHLkSLVr106zZ89WcXGxXn31VQ0ePFibNm1SUlKSJOnee+/VRx99pGnTpqlbt27KysrS6tWrtWvXLvXt21dlZWUaMWKESktLdf/99ysuLk5Hjx7VZ599ppycHIWFhTlUF+B1DABeJTc315BkjB49ukHtt2zZYkgy7rrrrhrbH3nkEUOS8c033xiGYRjHjx83/P39jWuuucaw2Wz2dr/73e8MScaECRPs2/7whz8YwcHBxs8//1zjmI8//rjh4+NjHD582DAMw7jhhhsMSUZOTk6NdsXFxcaJEyfsr+zsbPu+f/7zn4bZbDa+++67Gp9ZsGCBIclYs2aNfZskw9/f39i3b59929atWw1Jxquvvlrvv0tqaqohyVi4cKF9W+/evY2YmBgjKyurxvHMZrMxfvx4+7awsDBj6tSpdR578+bNhiTjww8/rLcGALXjthTgZfLy8iRJISEhDWr/xRdfSJJmzJhRY/vDDz8sSfaxOcuWLVNZWZnuv/9+mUwme7vp06efdcwPP/xQl112mSIiInTy5En7a9iwYbJarVq1alWNWlu0aFHj8wsWLFDLli3tr0svvbTGsbt27aouXbrUOPaVV14pSfr2229rHGvYsGFq3769/X2vXr0UGhqqAwcONOjfp1p6erq2bNmiiRMnKjIyssbxhg8fbv93lKTw8HBt2LBBx44dq/VY1T0zX331lYqKihyqAwBjbgCvExoaKqly3EdDHDp0SGazWR06dKixPS4uTuHh4Tp06JC9nSR17NixRruWLVsqIiKixra9e/dqyZIlNQJKy5YtNWzYMEnS8ePHJZ0OYAUFBTU+P3bsWC1dulRLly5Vr169zjr2zp07zzp2p06dahy7Wps2bc4654iICGVnZ5/jX6am6vPv3LnzWfu6du2qkydPqrCwUFLlmKcdO3YoMTFRF198sWbPnl0jTCUnJ2vGjBl64403FB0drREjRmjevHmMtwEaiDE3gJcJDQ1VQkKCduzY4dDnzuyNuVA2m03Dhw/Xo48+Wuv+6iDSpUsXSdKOHTs0ePBg+/7ExEQlJiZKkr3358xj9+zZU3Pnzq312NWfq+bj41NrO8MwGng2jhs3bpwuu+wyffLJJ/r666/15z//Wc8995w+/vhjjRw5UpL0wgsvaOLEifr000/19ddf64EHHtCcOXO0fv16tW7d2mW1AZ6AcAN4oWuvvVavvfaa1q1bp4EDB9bbtm3btrLZbNq7d6+6du1q356ZmamcnBy1bdvW3k6q7Dlp166dvd2JEyfO6gVp3769CgoK7D019dX57LPP6l//+leNcFOf9u3ba+vWrRo6dKhTA9m5VJ9/bdPSd+/erejoaAUHB9u3xcfHa8qUKZoyZYqOHz+uvn376o9//KM93EhSz5491bNnTz3xxBNau3atBg8erAULFuiZZ55x/QkBzRi3pQAv9Oijjyo4OFh33XWXMjMzz9q/f/9+vfzyy5KkUaNGSZJ9BlO16p6Ra665RlLl2BU/Pz+9+uqrNXo9fvk5qbLnonqa9y/l5OSooqJCkjR48GANHz5cr732mj799NNaz+WXPSzjxo3T0aNH9frrr5/Vtri42H5ryNni4+PVu3dvvf3228rJybFv37Fjh77++mv7v6PVaj3r9lJMTIwSEhLsU9Xz8vLs/wbVevbsKbPZfNZ0dgBno+cG8ELt27fXu+++q1tuuUVdu3at8YTitWvX6sMPP7SvBZWSkqIJEybotddeU05Oji6//HJt3LhRb7/9tsaMGaMhQ4ZIqhxb88gjj2jOnDm69tprNWrUKG3evFlffvmloqOja/z83/72t/rvf/+ra6+9VhMnTlS/fv1UWFio7du366OPPtLBgwftn3nnnXd09dVXa8yYMRo5cqSGDRumiIgI+xOKV61aVaO344477tAHH3yge++9V99++60GDx4sq9Wq3bt364MPPtBXX32l/v37u+Tf9c9//rNGjhypgQMH6s4777RPBQ8LC9Ps2bMlVY51at26tW666SalpKSoRYsWWrZsmb7//nu98MILkiqflTNt2jTdfPPN6tSpkyoqKvTPf/5TPj4+Gjt2rEtqBzyKm2drAXCjn3/+2bj77ruNpKQkw9/f3wgJCTEGDx5svPrqq0ZJSYm9XXl5ufH0008bycnJhp+fn5GYmGjMnDmzRhvDMAyr1Wo8/fTTRnx8vBEYGGhcccUVxo4dO4y2bdvWmApuGIaRn59vzJw50+jQoYPh7+9vREdHG4MGDTL+8pe/GGVlZTXaFhcXGy+99JIxcOBAIzQ01PD19TXi4uKMa6+91vjXv/5lVFRU1GhfVlZmPPfcc0b37t0Ni8ViREREGP369TOefvppIzc3195OUq1Tsmur95dqmwpuGIaxbNkyY/DgwUZgYKARGhpqXHfddcZPP/1k319aWmr89re/NVJSUoyQkBAjODjYSElJMf72t7/Z2xw4cMCYNGmS0b59eyMgIMCIjIw0hgwZYixbtqzemgBUMhmGC0fNAQAANDLG3AAAAI9CuAEAAB6FcAMAADwK4QYAAHgUwg0AAPAohBsAAOBRvO4hfjabTceOHVNISEijPpodAACcP8MwlJ+fr4SEBJnN9ffNeF24OXbs2FkL5wEAgOYhLS3tnIvHel24CQkJkVT5jxMaGurmagAAQEPk5eUpMTHR/ne8Pl4XbqpvRYWGhhJuAABoZhoypIQBxQAAwKMQbgAAgEch3AAAAI9CuAEAAB6FcAMAADwK4QYAAHgUwg0AAPAohBsAAOBRCDcAAMCjEG4AAIBHIdwAAACPQrgBAAAehXDjJIZh6GRBqfafKHB3KQAAeDXCjZOs+PmE+j+zTFP/tcndpQAA4NUIN07SNjJIknQoq0iGYbi5GgAAvBfhxkkSI4PkYzapuNyqzLxSd5cDAIDXItw4iZ+PWa0jAiVJqScL3VwNAADei3DjRElRwZKkg1mEGwAA3IVw40TJ0VXhhp4bAADchnDjRElRlYOKuS0FAID7EG6cKCma21IAALgb4caJqm9LHcoqks3GdHAAANyBcONErcID5Ws2qbTCpvS8EneXAwCAVyLcOJGvj1ltqh7mx6BiAADcg3DjZNXjbhhUDACAexBunMz+rBvCDQAAbkG4cbLk6KrbUsyYAgDALQg3TsZtKQAA3Itw42TVt6XSThXLynRwAAAaHeHGyRLCA+XvY1aZ1aZjOcXuLgcAAK9DuHEyH7NJbViGAQAAtyHcuACrgwMA4D6EGxeonjFFzw0AAI2PcOMC9gU0CTcAADQ6wo0LJNtvSxW5uRIAALwP4cYFqntu0k4VqcJqc3M1AAB4F8KNC8SFBsjia1aFzdCRbKaDAwDQmAg3LmA2m+wzplKZMQUAQKMi3LhIUvUaUwwqBgCgURFuXIQZUwAAuAfhxkWS7belmDEFAEBjIty4CD03AAC4B+HGRZKrws2R7CKVVTAdHACAxkK4cZGYEIuC/H1kM6S0bG5NAQDQWAg3LmIymdQ2iltTAAA0NsKNC7GAJgAAjY9w40JJ9jWmCDcAADQWwo0LnZ4xxZgbAAAaC+HGhdpVhRtuSwEA0HgINy5U3XNzLLdYJeVWN1cDAIB3INy4UFSwv0IsvjIM6fApbk0BANAYCDcuZDKZ7L033JoCAKBxEG5cjGUYAABoXIQbF0uOqnzWDdPBAQBoHIQbF+O2FAAAjYtw42I86wYAgMbl1nAzZ84cXXTRRQoJCVFMTIzGjBmjPXv2nPNzH374obp06aKAgAD17NlTX3zxRSNUe36Sq55SnJFXouIypoMDAOBqbg03K1eu1NSpU7V+/XotXbpU5eXluuqqq1RYWPctnLVr1+q2227TnXfeqc2bN2vMmDEaM2aMduzY0YiVN1xEsL/CAv0kMe4GAIDGYDIMw3B3EdVOnDihmJgYrVy5Ur/61a9qbXPLLbeosLBQn332mX3bJZdcot69e2vBggXn/Bl5eXkKCwtTbm6uQkNDnVZ7fUbPW6OtaTmaf3tfjewZ3yg/EwAAT+LI3+8mNeYmNzdXkhQZGVlnm3Xr1mnYsGE1to0YMULr1q2rtX1paany8vJqvBpb9YypVHpuAABwuSYTbmw2m6ZPn67BgwerR48edbbLyMhQbGxsjW2xsbHKyMiotf2cOXMUFhZmfyUmJjq17obgWTcAADSeJhNupk6dqh07dui9995z6nFnzpyp3Nxc+ystLc2px2+IZGZMAQDQaHzdXYAkTZs2TZ999plWrVql1q1b19s2Li5OmZmZNbZlZmYqLi6u1vYWi0UWi8VptZ6PpKoZU9yWAgDA9dzac2MYhqZNm6ZPPvlE33zzjZKTk8/5mYEDB2r58uU1ti1dulQDBw50VZkXrPq21In8UhWUVri5GgAAPJtbw83UqVP1zjvv6N1331VISIgyMjKUkZGh4uJie5vx48dr5syZ9vcPPviglixZohdeeEG7d+/W7Nmz9cMPP2jatGnuOIUGCQv0U2SwvyTG3QAA4GpuDTfz589Xbm6urrjiCsXHx9tf77//vr3N4cOHlZ6ebn8/aNAgvfvuu3rttdeUkpKijz76SIsXL653EHJTkMQaUwAANAq3jrlpyCN2VqxYcda2m2++WTfffLMLKnKdpOhgbTqcQ88NAAAu1mRmS3m66mUYUpkxBQCASxFuGon9WTfclgIAwKUIN40kmQf5AQDQKAg3jaS65yarsEx5JeVurgYAAM9FuGkkLSy+im5R+TBBem8AAHAdwk0jSo6uWkCTcAMAgMsQbhpR9TIMrDEFAIDrEG4aETOmAABwPcJNI6qeMcVtKQAAXIdw04jst6XouQEAwGUIN40oqWpAcU5RuXKKytxcDQAAnolw04iC/H0VG1o5HZxbUwAAuAbhppFxawoAANci3DSy04OKmQ4OAIArEG4aWRJrTAEA4FKEm0bGbSkAAFyLcNPIznzWjWEYbq4GAADPQ7hpZG2jKqeD55dU6FQh08EBAHA2wk0jC/DzUUJYgCRuTQEA4AqEGzdIYsYUAAAuQ7hxA2ZMAQDgOoQbN0iumjGVym0pAACcjnDjBvTcAADgOoQbN0iuWkDzINPBAQBwOsKNGyRGBslskgrLrDpRUOrucgAA8CiEGzew+PooITxQknSQGVMAADgV4cZNkhl3AwCASxBu3CSJGVMAALgE4cZNmDEFAIBrEG7cpHrGVCrhBgAApyLcuEn1balDWUVMBwcAwIkIN26SGBkkH7NJxeVWZeYxHRwAAGch3LiJn49ZrSMqp4NzawoAAOch3LhR9a2pg8yYAgDAaQg3bsSzbgAAcD7CjRslRTFjCgAAZyPcuJH9WTfclgIAwGkIN25UfVvqUFaRbDamgwMA4AyEGzdqFR4oX7NJpRU2peeVuLscAAA8AuHGjXx9zGoTWTnuhkHFAAA4B+HGzarH3TCoGAAA5yDcuJn9WTeEGwAAnIJw42bVC2gyYwoAAOcg3LgZt6UAAHAuwo2bVd+WSjtVLCvTwQEAuGCEGzdLCA+Uv49ZZVabjuUUu7scAACaPcKNm/mYTWrDMgwAADgN4aYJYHVwAACch3DTBFTPmKLnBgCAC0e4aQLsC2gSbgAAuGCEmyYg2X5bqsjNlQAA0PwRbpqA6p6btFNFqrDa3FwNAADNm1PCTU5OjjMO47XiQgNk8TWrwmboSDbTwQEAuBAOh5vnnntO77//vv39uHHjFBUVpVatWmnr1q1OLc5bmM0m+4ypVGZMAQBwQRwONwsWLFBiYqIkaenSpVq6dKm+/PJLjRw5Ur/97W+dXqC3SKpeY4pBxQAAXBBfRz+QkZFhDzefffaZxo0bp6uuukpJSUkaMGCA0wv0FsyYAgDAORzuuYmIiFBaWpokacmSJRo2bJgkyTAMWa1W51bnRZLtt6WYMQUAwIVwuOfmxhtv1K9//Wt17NhRWVlZGjlypCRp8+bN6tChg9ML9Bb03AAA4BwOh5sXX3xRSUlJSktL0/PPP68WLVpIktLT0zVlyhSnF+gtkqvCzZHsIpVV2OTvyyx9AADOh8Phxs/PT4888shZ2x966CGnFOStYkIsCvL3UVGZVWnZRWrfsoW7SwIAoFlyuHvg7bff1ueff25//+ijjyo8PFyDBg3SoUOHnFqcNzGZTGobxa0pAAAulMPh5k9/+pMCAwMlSevWrdO8efP0/PPPKzo6mt6bC9Su6tYUC2gCAHD+HL4tlZaWZh84vHjxYo0dO1aTJ0/W4MGDdcUVVzi7Pq+SxOrgAABcMId7blq0aKGsrCxJ0tdff63hw4dLkgICAlRczNIBFyLJvoAm4QYAgPPlcM/N8OHDddddd6lPnz76+eefNWrUKEnSzp07lZSU5Oz6vEqyfTo4z7oBAOB8OdxzM2/ePA0cOFAnTpzQf/7zH0VFRUmSfvzxR912221OL9CbVD/r5lhusUrKeSAiAADnw+FwEx4err/+9a/69NNPdfXVV9u3P/300/r973/v0LFWrVql6667TgkJCTKZTFq8eHG97VesWCGTyXTWKyMjw9HTaJKigv0VYvGVYUiHT9F7AwDA+XD4tpQk5eTk6M0339SuXbskSd27d9ekSZMUFhbm0HEKCwuVkpKiSZMm6cYbb2zw5/bs2aPQ0FD7+5iYGId+blNlMpmUFB2s7UdzlXqyUJ1iQ9xdEgAAzY7D4eaHH37QiBEjFBgYqIsvvliSNHfuXP3xj3/U119/rb59+zb4WCNHjrQv3+CImJgYhYeHO/y55qA63PCsGwAAzo/Dt6UeeughXX/99Tp48KA+/vhjffzxx0pNTdW1116r6dOnu6DEs/Xu3Vvx8fEaPny41qxZ0yg/s7EkR1VOB2fGFAAA5+e8em5ef/11+fqe/qivr68effRR9e/f36nF/VJ8fLwWLFig/v37q7S0VG+88YauuOIKbdiwoc4eo9LSUpWWltrf5+XlubTGC5XEg/wAALggDoeb0NBQHT58WF26dKmxPS0tTSEhrh0j0rlzZ3Xu3Nn+ftCgQdq/f79efPFF/fOf/6z1M3PmzNHTTz/t0rqcKYnp4AAAXBCHb0vdcsstuvPOO/X+++8rLS1NaWlpeu+993TXXXe5ZSr4xRdfrH379tW5f+bMmcrNzbW/0tLSGrE6xyVXPcgvI69ExWVMBwcAwFEO99z85S9/kclk0vjx41VRUSGpcqXw++67T88++6zTCzyXLVu2KD4+vs79FotFFoulESu6MBHB/goL9FNucbkOZhWqa3zouT8EAADsHA43/v7+evnllzVnzhzt379fktS+fXsFBQU5/MMLCgpq9LqkpqZqy5YtioyMVJs2bTRz5kwdPXpU//jHPyRJL730kpKTk9W9e3eVlJTojTfe0DfffKOvv/7a4Z/dlCVFB2trWo4OniTcAADgqPN6zo0kBQUFqWfPnhf0w3/44QcNGTLE/n7GjBmSpAkTJmjRokVKT0/X4cOH7fvLysr08MMP6+jRowoKClKvXr20bNmyGsfwBMlRQdqalqNUZkwBAOAwk2EYxrkaOfKAvY8//viCCnK1vLw8hYWFKTc3t8aDAJuSl5b9rJeW7dW4/q31/E0p7i4HAAC3c+Tvd4N6bhx98jAuDAtoAgBw/hoUbhYuXOjqOnCGpKoZU9yWAgDAcQ5PBYfrVT/r5kR+qQpKK9xcDQAAzQvhpgkKC/RTZLC/JLHGFAAADiLcNFFJrDEFAMB5Idw0UaeXYSDcAADgCIfDzYEDB1xRB36hehmGVGZMAQDgEIfDTYcOHTRkyBC98847KikpcUVN0Bk9N9yWAgDAIQ6Hm02bNqlXr16aMWOG4uLidM8992jjxo2uqM2rJXNbCgCA8+JwuOndu7defvllHTt2TG+99ZbS09N16aWXqkePHpo7d65OnDjhijq9TnXPTVZhmfJKyt1cDQAAzcd5Dyj29fXVjTfeqA8//FDPPfec9u3bp0ceeUSJiYkaP3680tPTnVmn12lh8VV0i8rVzOm9AQCg4c473Pzwww+aMmWK4uPjNXfuXD3yyCPav3+/li5dqmPHjmn06NHOrNMrJUdXTgdPJdwAANBgDq8KPnfuXC1cuFB79uzRqFGj9I9//EOjRo2S2VyZk5KTk7Vo0SIlJSU5u1avkxQVrO8PZrPGFAAADnA43MyfP1+TJk3SxIkTFR8fX2ubmJgYvfnmmxdcnLdjxhQAAI5zONzs3bv3nG38/f01YcKE8yoIp1XPmOK2FAAADedwuJGk7Oxsvfnmm9q1a5ckqWvXrpo0aZIiIyOdWpy3q14dnJ4bAAAazuEBxatWrVJSUpJeeeUVZWdnKzs7W6+++qqSk5O1atUqV9TotZKqBhTnFJUrp6jMzdUAANA8ONxzM3XqVN1yyy2aP3++fHx8JElWq1VTpkzR1KlTtX37dqcX6a2C/H0VG2pRZl6pUk8Wqk8bf3eXBABAk+dwz82+ffv08MMP24ONJPn4+GjGjBnat2+fU4sDt6YAAHCUw+Gmb9++9rE2Z9q1a5dSUlKcUhROOz2omOngAAA0hMO3pR544AE9+OCD2rdvny655BJJ0vr16zVv3jw9++yz2rZtm71tr169nFepl0pijSkAABzicLi57bbbJEmPPvporftMJpMMw5DJZJLVar3wCr0ct6UAAHCMw+EmNTXVFXWgDmc+66Y6NAIAgLo5HG7atm3rijpQh7ZRldPB80sqdKqwTFFVi2kCAIDanddD/Pbv36+XXnrJPrC4W7duevDBB9W+fXunFgcpwM9HCWEBOpZbooNZhYQbAADOweHZUl999ZW6deumjRs3qlevXurVq5c2bNig7t27a+nSpa6o0eslMWMKAIAGc7jn5vHHH9dDDz2kZ5999qztjz32mIYPH+604lApKTpYa/dnMWMKAIAGcLjnZteuXbrzzjvP2j5p0iT99NNPTikKNSVXzZhKZcYUAADn5HC4admypbZs2XLW9i1btigmJsYZNeEXeNYNAAAN5/BtqbvvvluTJ0/WgQMHNGjQIEnSmjVr9Nxzz2nGjBlOLxBSctUCmgeZDg4AwDk5HG6efPJJhYSE6IUXXtDMmTMlSQkJCZo9e7YeeOABpxcIKTEySGaTVFhm1YmCUsWEBLi7JAAAmiyHwk1FRYXeffdd/frXv9ZDDz2k/Px8SVJISIhLikMli6+PEsIDdSS7WAdPFhFuAACoh0Njbnx9fXXvvfeqpKREUmWoIdg0jmTG3QAA0CAODyi++OKLtXnzZlfUgnpUrzF1gHADAEC9HB5zM2XKFD388MM6cuSI+vXrp+Dg4Br7WQncNTrHVfaQbTuS495CAABo4hwON7feeqsk1Rg8zErgrjcgOVKStOlwtkorrLL4+ri5IgAAmiZWBW8mOsS0UFSwv7IKy7TtSK4uSop0d0kAADRJDo+5OXTokFq1aqW2bdvWeLVq1UqHDh1yRY1QZe/YgHaVgWbDgSw3VwMAQNPlcLgZMmSITp06ddb23NxcDRkyxClFoXYDkqMkSRtSz/73BwAAlRwON3U9ITcrK+uswcVwruqemx8PZavcanNzNQAANE0NHnNz4403Sqq8PTJx4kRZLBb7PqvVqm3bttmXY4BrdIoJUXiQn3KKyrX9aK76tolwd0kAADQ5DQ43YWFhkip7bkJCQhQYGGjf5+/vr0suuUR333238yuEndls0sVJkfr6p0ytP5BFuAEAoBYNDjcLFy6UJCUlJemRRx7hFpSbXNIuSl//lKkNB05pyhXurgYAgKbH4angs2bNckUdaKDqcTc/HDylCqtNvj4OD5sCAMCjOfyXMTMzU3fccYcSEhLk6+srHx+fGi+4Vpe4UIUG+KqwzKqdx/LcXQ4AAE2Owz03EydO1OHDh/Xkk08qPj6+1plTcB0fs0kXJ0dq2a7j2pCapZTEcHeXBABAk+JwuFm9erW+++479e7d2wXloCEGJEdp2a7jWn/glCb/qr27ywEAoElx+LZUYmKiDMNwRS1ooEvaVT7M7/vUU7LauBYAAJzJ4XDz0ksv6fHHH9fBgwddUA4aoltCqEIsvsovrdCudMbdAABwJodvS91yyy0qKipS+/btFRQUJD8/vxr7a1uaAc7lYzapf1KEvt1zQusPZKlHqzB3lwQAQJPhcLh56aWXXFAGHDWgXZS+3XNCG1JP6a7L2rm7HAAAmgyHw82ECRNcUQccNCC58nk3G1NPyWYzZDYzaw0AAOk8xtxI0v79+/XEE0/otttu0/HjxyVJX375pXbu3OnU4lC3Hq3CFOzvo9zicu3OyHd3OQAANBkOh5uVK1eqZ8+e2rBhgz7++GMVFBRIkrZu3crTixuRn49Z/ZIqe282pGa5uRoAAJoOh8PN448/rmeeeUZLly6Vv7+/ffuVV16p9evXO7U41K/61tSGAwziBgCgmsPhZvv27brhhhvO2h4TE6OTJ086pSg0zCVV60xtPFg57gYAAJxHuAkPD1d6evpZ2zdv3qxWrVo5pSg0TM9W4QrwM+tUYZn2Hi9wdzkAADQJDoebW2+9VY899pgyMjJkMplks9m0Zs0aPfLIIxo/frwrakQd/H3N6t+WcTcAAJzJ4XDzpz/9SV26dFFiYqIKCgrUrVs3/epXv9KgQYP0xBNPuKJG1INxNwAA1OTwc278/f31+uuv66mnntL27dtVUFCgPn36qGPHjq6oD+cwoGqdqQ2pWTIMg1XaAQBez+FwUy0xMVGHDx/W0KFDZbFYnFkTHJCSGCaLr1knC8q0/0ShOsS0cHdJAAC41Xk9xK/ayJEjdfToUWfVgvNg8fVR3zYRkqT1Bxh3AwDABYUbw2D6cVMwoF31oGLG3QAAcEHhBk3DgOSqcTcHsgicAACvd0Hh5u9//7tiY2OdVQvOU5824fL3Met4fqkOZhW5uxwAANzqgsLNr3/9a1mtVi1evFi7du1yVk1wUICfj3onhkuq7L0BAMCbORxuxo0bp7/+9a+SpOLiYvXv31/jxo1Tr1699J///MfpBaJhqpdiYFAxAMDbORxuVq1apcsuu0yS9Mknn8gwDOXk5OiVV17RM8884/CxrrvuOiUkJMhkMmnx4sXn/MyKFSvUt29fWSwWdejQQYsWLXL0FDzS6efdnGLcDQDAqzkcbnJzcxUZWdlLsGTJEo0dO1ZBQUG65pprtHfvXoeOVVhYqJSUFM2bN69B7VNTU3XNNddoyJAh2rJli6ZPn6677rpLX331laOn4XH6tomQn49J6bklSjtV7O5yAABwG4cf4peYmKh169YpMjJSS5Ys0XvvvSdJys7OVkBAgEPHGjlypEaOHNng9gsWLFBycrJeeOEFSVLXrl21evVqvfjiixoxYoRDP9vTBPr7qFfrcP14KFvrU7PUJirI3SUBAOAWDvfcTJ8+Xbfffrtat26thIQEXXHFFZIqbzH17NnT2fXVsG7dOg0bNqzGthEjRmjdunV1fqa0tFR5eXk1Xp6KdaYAADiPcDNlyhStW7dOb731llavXi2zufIQ7dq1c3jMjaMyMjLOmnoeGxurvLw8FRfXfitmzpw5CgsLs78SExNdWqM7XVI17oZBxQAAb3ZeU8H79++vG264QS1atJDVatWWLVs0aNAgDR482Nn1XbCZM2cqNzfX/kpLS3N3SS7Tr22EfMwmHc0p1pFsnncDAPBO53Vb6s0335QkWa1WXX755erbt68SExO1YsUKZ9dXQ1xcnDIzM2tsy8zMVGhoqAIDA2v9jMViUWhoaI2Xpwq2+KpnqzBJ3JoCAHgvh8PNRx99pJSUFEnS//73P6Wmpmr37t166KGH9Pvf/97pBZ5p4MCBWr58eY1tS5cu1cCBA136c5uT0+tMcWsKAOCdHA43J0+eVFxcnCTpiy++0M0336xOnTpp0qRJ2r59u0PHKigo0JYtW7RlyxZJlVO9t2zZosOHD0uqvKU0fvx4e/t7771XBw4c0KOPPqrdu3frb3/7mz744AM99NBDjp6Gx7ok+fTzbgAA8EYOh5vY2Fj99NNPslqtWrJkiYYPHy5JKioqko+Pj0PH+uGHH9SnTx/16dNHkjRjxgz16dNHTz31lCQpPT3dHnQkKTk5WZ9//rmWLl2qlJQUvfDCC3rjjTe8fhr4mfonRchskg5lFSk9l+fdAAC8j8PPufnNb36jcePGKT4+XiaTyT41e8OGDerSpYtDx7riiivqfZpubU8fvuKKK7R582aHfo43CQnwU49WYdp2JFcbDpzSmD6t3F0SAACNyuFwM3v2bPXo0UNpaWm6+eabZbFYJEk+Pj56/PHHnV4gHDcgObIy3KRmEW4AAF7H4XAjSTfddNNZ2yZMmHDBxcA5BiRH6fXvUpkxBQDwSuf1nJuVK1fquuuuU4cOHdShQwddf/31+u6775xdG87TRcmRMpmkAycLdTyvxN3lAADQqBwON++8846GDRumoKAgPfDAA3rggQcUGBiooUOH6t1333VFjXBQWKCfusVXPs9nPbOmAABexmTUN6K3Fl27dtXkyZPPmn49d+5cvf7669q1a5dTC3S2vLw8hYWFKTc316Mf6Pf//veT3lqTqtsHtNEfb3Dtml8AALiaI3+/He65OXDggK677rqztl9//fVKTU119HBwkdMP86PnBgDgXRwON4mJiWc9JViSli1b5tGLUjY3FydVhpt9xwt0sqDUzdUAANB4HJ4t9fDDD+uBBx6wL5YpSWvWrNGiRYv08ssvO71AnJ+IYH91iQvR7ox8bUw9pVE9491dEgAAjcLhcHPfffcpLi5OL7zwgj744ANJleNw3n//fY0ePdrpBeL8XdIuSrsz8rX+QBbhBgDgNRwKNxUVFfrTn/6kSZMmafXq1a6qCU4yIDlSi9Ye5Hk3AACv4tCYG19fXz3//POqqKhwVT1woouTK8fd7MnM16nCMjdXAwBA43B4QPHQoUO1cuVKV9QCJ4tqYVHHmBaSpI3MmgIAeAmHx9yMHDlSjz/+uLZv365+/fopODi4xv7rr7/eacXhwl3SLkp7jxdoQ2qWru4R5+5yAABwOYfDzZQpUyRVPrTvl0wmk6xW64VXBacZ0C5S/1x/SOsZdwMA8BIOhxubzeaKOuAi1eNudmfkKbeoXGFBfm6uCAAA1zqvhTPRfMSEBKhdy2AZhrTxIL03AADP1+Bw880336hbt27Ky8s7a19ubq66d++uVatWObU4OMeA5ChJ0oYDWW6uBAAA12twuHnppZd0991317pYVVhYmO655x69+OKLTi0OznEJ60wBALxIg8PN1q1bdfXVV9e5/6qrrtKPP/7olKLgXJe0q+y52XksV3kl5W6uBgAA12pwuMnMzJSfX92DUX19fXXixAmnFAXnig0NUFJUkGyG9APjbgAAHq7B4aZVq1basWNHnfu3bdum+HjWL2qqTo+7IdwAADxbg8PNqFGj9OSTT6qkpOSsfcXFxZo1a5auvfZapxYH5xlQNe5mPeNuAAAersHPuXniiSf08ccfq1OnTpo2bZo6d+4sSdq9e7fmzZsnq9Wq3//+9y4rFBdmQNW4mx1Hc1VQWqEWFocfcQQAQLPQ4L9wsbGxWrt2re677z7NnDlThmFIqnwq8YgRIzRv3jzFxsa6rFBcmFbhgUqMDFTaqWL9cPCUrugc4+6SAABwCYf+971t27b64osvlJ2drX379skwDHXs2FERERGuqg9ONCA5SmmnjmhDKuEGAOC5zuveREREhC666CJn1wIXG5AcqY9+PMLD/AAAHo3lF7xI9fNuth3JVVFZhZurAQDANQg3XqR1RKBahQeqwmZo06Ecd5cDAIBLEG68iMlk0oCqVcLXc2sKAOChCDdeZoB9nSnCDQDAMxFuvEz1k4q3puWqpNzq5moAAHA+wo2XaRsVpNhQi8qsNm06nO3ucgAAcDrCjZcxmUz2WVOsMwUA8ESEGy9UfWuKQcUAAE9EuPFC1YOKN6flMO4GAOBxCDdeqF10sKJbWFRWYdPWtBx3lwMAgFMRbryQyWQ6Y0o4424AAJ6FcOOl7IOKed4NAMDDEG681CVVTyr+8VC2yipsbq4GAADnIdx4qQ4xLRQV7K+Scpu2HclxdzkAADgN4cZLmUwmXZzMuBsAgOch3Hix6nE3PO8GAOBJCDderHrG1I+HslVuZdwNAMAzEG68WKeYEIUH+amozKrtR3PdXQ4AAE5BuPFiZrNJFydVjbthnSkAgIcg3Hi5ATzvBgDgYQg3Xu6SqnE3PxzMVgXjbgAAHoBw4+W6xIUqNMBXBaUV+ik9z93lAABwwQg3Xs7HfPp5N0wJBwB4AsINNCC5atwNg4oBAB6AcAP78242Hjwlq81wczUAAFwYwg3ULT5UIRZf5ZdU6H9bj7m7HAAALgjhBvL1MWt491hJ0vT3t2jyP37QsZxiN1cFAMD5IdxAkvTHMT117+Xt5Ws26eufMjVs7kq98d0BpocDAJodwg0kSYH+Pnp8ZBd99sCl6t82QkVlVj3z+S5d99c12nQ4293lAQDQYIQb1NAlLlQf3DNQz43tqfAgP+1Kz9PY+Wv1u0+2K7eo3N3lAQBwToQbnMVsNumWi9po+YzLdVO/1jIM6d0NhzV07gp9svmIDIMZVQCApotwgzpFtbDoLzen6L3Jl6hDTAudLCjTQ+9v1e1vbND+EwXuLg8AgFoRbnBOl7SL0hcPXKbfjugsi69Za/dnaeRL32nu0p9VUm51d3kAANRAuEGD+PuaNXVIBy196HJd0bmlyqw2vbJ8r65+aZW+23vC3eUBAGBHuIFD2kQFaeHEi/S32/sqNtSig1lFuuPNjXrg35t1PL/E3eUBAEC4geNMJpNG9YzXshmX6zeDk2Q2Sf/dekxD/7JS/1x3kCUcAABuZTK8bOpLXl6ewsLClJubq9DQUHeX4xF2HM3V7z7Zrm1HciVJKa3D9McbeqpHqzA3VwYA8BSO/P2m5wYXrEerMH0yZbD+3+juCrH4auuRXF3/19V6+n87lV/Cs3EAAI2LcAOn8DGbNH5gkpY/fLmuS0mQzZAWrjmoYXNX6ovt6TwbBwDQaAg3cKqY0AC9elsf/WPSxWobFaTMvFJN+dcmTVr0vdJOFbm7PACAF2gS4WbevHlKSkpSQECABgwYoI0bN9bZdtGiRTKZTDVeAQEBjVgtGuJXnVrqq+m/0gNDO8rfx6xv95zQ8BdX6vVVLMYJAHAtt4eb999/XzNmzNCsWbO0adMmpaSkaMSIETp+/HidnwkNDVV6err9dejQoUasGA0V4OejGcM76cvpl2lguyiVlNv0xy926Ya/rdXOY7nuLg8A4KHcHm7mzp2ru+++W7/5zW/UrVs3LViwQEFBQXrrrbfq/IzJZFJcXJz9FRsb24gVw1HtW7bQu3cP0PNjeyk0wFfbj+bq+r+u0bNf7uYJxwAAp3NruCkrK9OPP/6oYcOG2beZzWYNGzZM69atq/NzBQUFatu2rRITEzV69Gjt3LmzzralpaXKy8ur8ULjM5lMGndRopY9fLmu6Rkvq83QgpX7dfVLq7R2/0l3lwcA8CBuDTcnT56U1Wo9q+clNjZWGRkZtX6mc+fOeuutt/Tpp5/qnXfekc1m06BBg3TkyJFa28+ZM0dhYWH2V2JiotPPAw0XExKgebf31Wt39FNcaIAOZhXp169v0OP/2abcIqaNAwAunNtvSzlq4MCBGj9+vHr37q3LL79cH3/8sVq2bKm///3vtbafOXOmcnNz7a+0tLRGrhi1uap7nJbO+JXuuKStJOm979M0lGnjAAAncGu4iY6Olo+PjzIzM2tsz8zMVFxcXIOO4efnpz59+mjfvn217rdYLAoNDa3xQtMQEuCnP4zpoQ/vHaj2LYN1sqBy2vjkf/6ojFzWqQIAnB+3hht/f3/169dPy5cvt2+z2Wxavny5Bg4c2KBjWK1Wbd++XfHx8a4qEy52UVKkvnjwMj0wtKP8fExa+lOmhs9dqXfWH5KNdaoAAA5y+22pGTNm6PXXX9fbb7+tXbt26b777lNhYaF+85vfSJLGjx+vmTNn2tv/v//3//T111/rwIED2rRpk/7v//5Phw4d0l133eWuU4ATWHwrp41/dv9l6p0YrvzSCj2xeIdueW2d9h0vcHd5AIBmxNfdBdxyyy06ceKEnnrqKWVkZKh3795asmSJfZDx4cOHZTafzmDZ2dm6++67lZGRoYiICPXr109r165Vt27d3HUKcKLOcSH6z32D9M91B/X8V3v0/cFsjXr5O91/ZQfdc3l7+fu6PY8DAJo4VgVHk3U0p1i//2S7Vuw5IUnqHBuiZ8f2VJ82EW6uDADQ2FgVHB6hVXigFk68SC/f2luRwf7ak5mvG+ev1dP/26nC0gp3lwcAaKIIN2jSTCaTRvdupWUzLteNfVvJqFpt/KoXV+nbPXUv0QEA8F6EGzQLkcH+mjuut/4x6WK1jgjU0Zxi/Wbh95r+3mZlFZS6uzwAQBNCuEGz8qtOLfX1Q7/SXZcmy2ySFm85pmFzV+qTzUd4+B8AQBIDit1dDi7A1rQcPfafbdqdkS9J6hofqutS4nVdrwQlRga5uToAgDM58vebcINmrdxq02urDuiV5XtVWmGzb09pHaZreyXoml7xSggPdGOFAABnINzUg3DjmbILy7RkZ4Y+23ZM6/Zn6cwHG/dvG6Fre8VrVK94xYQEuK9IAMB5I9zUg3Dj+U7kl2rJjnT9b1u6vj94StX/hZtM0oDkSF2XkqCRPeIVGezv3kIBAA1GuKkH4ca7ZOSW6PPt6fps2zFtPpxj3+5jNmlQ+yhd1ytBI7rHKSzIz31FAgDOiXBTD8KN9zqSXaTPt6Xrs23p2n40177dz8ekyzq21HUp8RrWNVYhAQQdAGhqCDf1INxAkg6eLNRn247ps23p9tlWkuTva9aQzi11ba8EDe0aoyB/ty+/BgAQ4aZehBv80r7j+frf1spbV/tPFNq3B/r56MquMbquV7yu6ByjAD8fN1YJAN6NcFMPwg3qYhiGdqXn23t0Dp8qsu9rYfHV5Z1banjXWA3pHMMYHQBoZISbehBu0BCGYWj70Vx9ti1dn29L19GcYvs+H7NJFydFani3WA3vFssDAwGgERBu6kG4gaNsNkPbjuZq6U8ZWvbTce3JzK+xv0tciIZ3i9WwrrHq2SpMZrPJTZUCgOci3NSDcIMLdSirUMt2HdfSnzL0/cFsWc94YmBsqEVDu1b26AxsF8U4HQBwEsJNPQg3cKacojJ9u+e4lv6UqZV7TqiwzGrfF+Tvo8s7tdSwrrG6skuMInhoIACcN8JNPQg3cJXSCqvW7c/Ssl2ZWvbTcWXkldj3mU1S/6RIXVV1+yopOtiNlQJA80O4qQfhBo3BMAztOJqnpT9l6OufMms8S0eSOsa00LCqAcm9W4czTgcAzoFwUw/CDdwh7VRRZY/OrkxtOHBKFWeM04luYdHlnVqqa3yIOsaGqGNMC8WHBchkIvAAQDXCTT0IN3C33OJyrThjnE5+acVZbUIsvuoQ20KdYkLUMbaFOsaGqFNsC8WFEnoAeCfCTT0IN2hKyips2pCape8PZmvf8Xz9nFmggycLa/TsnOmXoadTbIg6xYYoNtRC6AHg0Qg39SDcoKkrq7Ap9WSh9laFnb2Z+fo5M18Hs4pqTDs/U0iArzrGVIad6ltbhB4AnoRwUw/CDZqr6tDzc2Z+VeAp0N7j5w49nWJD1DU+RH0SI9SnTbiSo4MJPACaHcJNPQg38DSlFdaq0FOgfVWh5+fj+TpUR+gJD/JTSutw9WkTrj5tItS7dThrZQFo8gg39SDcwFtUh549GfnacTRXmw/naPvRXJVW2M5q275lsHpX9ez0aROuzrEh8vUxu6FqAKgd4aYehBt4s7IKm3Zn5Gnz4RxtScvR5sPZOphVdFa7QD8f9WwdVhl2EiPUt024YkID3FAxAFQi3NSDcAPUdKqwTFvSsrXlcI42p+Voy+GcWqentwoPVO/EcHvvTveEMNbOAtBoCDf1INwA9bPZDO0/UaDNaTnafLiyd+fnzHz9cviOn49JXeND1TuxcpByXGiA4sIqXy1bWLitBcCpCDf1INwAjisordC2I9W3sipfJwtK62xvNkktQyyKCw1Q7BmhJy40oEYICvL3bcSzANCcEW7qQbgBLpxhGDqSXawtaTnadiRHR3OKlZ5boszcEh3PL63zIYS/FBLgq/iwqgAUGlD5fVUIiq16Hxnsz9R1AISb+hBuANey2QydLCxVRm6JMnJLlJlXovTcEmXkVX5fvb2wzNqg4/n7mtUmMkhtI4PUNipYSdFBahMZpKSoYLWKCJQft78Ar+DI32/6hAE4ldlsUkxIgGJCAtSrdd3t8kvKq8JOqdJziyu/rw4/VduzCktVVmHTvuMF2ne84Kxj+JhNah0RaA87baOqAlBUkBIjgxjwDHgpem4ANFllFTZl5pXoUFaRDmYV6lBWoQ5lFVW+ThWqpPzsZ/ZUM5mkuNAAtY2qDj7V4acyALWw8P92QHPCbal6EG4Az2AYho7nl+rgycLT4edUUWUAOllU63T2M0W38FfbqGAlRgQqJjRAMSEWtQyxKLbq+5jQAAIQ0IQQbupBuAE8n2EYOlVYZg87B09WhZ5Tlb0+pwrLGnScIH+fyqATEqCY0DO/nv4+NiRAoYG+DHoGXIwxNwC8mslkUlQLi6JaWNS3TcRZ+/NKynW4qrfnaHaxjueXVr7ySuxfC8usKiqz6mBWUa1PcT6Tv6+5KvCcEXpCA9QyxKL4sAAlhAeqVXggY4CARkK4AeB1QgP81KNVmHq0CquzTWFpRY3Ak5lXohPVISi/RMfzKr/PLS5XWYVNR7KLdSS7uN6fGxXsr1YRgUoIC6z8WhV6WoVXvo8I8qMHCHACwg0A1CLY4qtki6+So4PrbVdSbq0KPacDT/X3mfmlysgt1tHsYhWWWZVVWKaswjJtO5Jb67EC/XyUEF7Z09O6lhAUFxbA1HegAQg3AHABAvx8lBhZOfW8LoZhKK+4QkdyinQsp0RHs4t0LLdER7OLdTSn8nUiv1TF5VbtP1Go/ScKaz2O2STFhp6+zRUfHqDIIH+FBvopLNBPoQF+Cg30rfrqp9AAX5bBgFdiQDEANAGlFVal55ToWE6xjuQU61hOZY/Psaqen2M5JSqz1j31vS7B/j5VQacy+JwOQZXhJ/TM91XBqLpNSICvzGZuk6FpYEAxADQzFl8fJUUHK6mO22DVT34+VhWAqoNPblG58krKlVdcUfW1XHklFSqomgpfWGZVYZlV6bklDtdUvUZYbNVyGLGhleuFxYSeXiIjLpTZYmh6CDcA0Ayc+eTn3onh52xfYbUpv6TirOCTW3x2GMqtCkR5Z+wrLrfKZkiZeaXKzCuVVPs4IUkK8DNXhp+QyrXBYkMsigs7MwRVBiRmi6GxEG4AwAP5+pgVEeyviGD/8/p8WYVNOUVlVeGmckmM43nVa4RVbsvMK1F2UblKym32J0fXJyzQr6rnx2L/GhlsUXQLf0UFWxQZ7K/oFpU1M3AaF4JwAwA4i7+vufLJzaEB6qm6p8yXlFurZoadXig1syoAnRmISsptyq3qJdqTmX/Onx8e5FcZdoItimrhr6gW/rUGoagWFoUH+jE2CDUQbgAA5y3Az0dtooLUJuocs8VKKuzBpzoEnSwo08mCUmUVlOlUYZmyCkt1qrBMNkPKKSpXTlG5DtQxc+xMZpMUGVwZeipDkL+iW1gUFlg5KNo+eDrATyFVA6tDqgZM00PkmQg3AACXMplMCquart4pNqTetlabodzicmUVlOpkwenAc7KgTFkFld9nFZTpZGFlKMotLpfNUFVQKpMyHast0M/HHnZCA06HntDqYBRwelZZyC8CUguLr4L9mVHWFBFuAABNho/ZpMjgyt6XjrHnbl9utSm7Ovz8IgjlFpfbB1XnVw2Yrn5fVGaVJBWXW1Vcbq0aNH1+Wlh8FRJQGXZaVH21v7f4qUWAr0Jr3e9nf9/C4isfQpLTEG4AAM2Wn8/psUGOqJ5NZp9RVjVLLL+kcuZYfo33tYekClvlY+IKSk9Pvb8Qwf4+9rATbPFVgK+PLH5mBfj5yOJb82uAn1kW35pfq/dbzmxXyzECfM0e/3BHwg0AwOtc6GwywzBUWlEZkApKK1RQUqH80srQU1C9rbSian+5fVt1oDq9v1zl1sqQVP1Mokydfy9SQ/maTVUh6XQwCjzj+1++D/TzkeWMbYH2z/7y85Vfgy0+imphcfl51Hl+bvvJAAA0UybT6XDQMuTC/oiXVlhrhJ+C0goVllaopNym0gqrSsptKim3qrTil1+tKi23qaSWr2d+trTcqpIKm8oqTj/husJmOK3HqTYprcP06bRLXXLshiDcAADgRhZfH1lauL6nw2YzVGatDEbVgam43Gr/Wlpus78vqfG99Yy2tqq2Ndv88vNB/u6NF4QbAAC8gNlsUoDZxyueFO3ZI4oAAIDXIdwAAACPQrgBAAAehXADAAA8CuEGAAB4FMINAADwKIQbAADgUQg3AADAoxBuAACARyHcAAAAj0K4AQAAHoVwAwAAPArhBgAAeBTCDQAA8Ci+7i6gsRmGIUnKy8tzcyUAAKChqv9uV/8dr4/XhZv8/HxJUmJiopsrAQAAjsrPz1dYWFi9bUxGQyKQB7HZbDp27JhCQkJkMpmceuy8vDwlJiYqLS1NoaGhTj12U8O5ei5vOl/O1XN50/l6y7kahqH8/HwlJCTIbK5/VI3X9dyYzWa1bt3apT8jNDTUo/8DOxPn6rm86Xw5V8/lTefrDed6rh6bagwoBgAAHoVwAwAAPArhxoksFotmzZoli8Xi7lJcjnP1XN50vpyr5/Km8/Wmc20orxtQDAAAPBs9NwAAwKMQbgAAgEch3AAAAI9CuAEAAB6FcOOgefPmKSkpSQEBARowYIA2btxYb/sPP/xQXbp0UUBAgHr27KkvvviikSo9f3PmzNFFF12kkJAQxcTEaMyYMdqzZ0+9n1m0aJFMJlONV0BAQCNVfGFmz559Vu1dunSp9zPN8bpKUlJS0lnnajKZNHXq1FrbN6frumrVKl133XVKSEiQyWTS4sWLa+w3DENPPfWU4uPjFRgYqGHDhmnv3r3nPK6jv/ONpb7zLS8v12OPPaaePXsqODhYCQkJGj9+vI4dO1bvMc/nd6ExnOvaTpw48ay6r7766nMetyle23Oda22/vyaTSX/+85/rPGZTva6uRLhxwPvvv68ZM2Zo1qxZ2rRpk1JSUjRixAgdP3681vZr167VbbfdpjvvvFObN2/WmDFjNGbMGO3YsaORK3fMypUrNXXqVK1fv15Lly5VeXm5rrrqKhUWFtb7udDQUKWnp9tfhw4daqSKL1z37t1r1L569eo62zbX6ypJ33//fY3zXLp0qSTp5ptvrvMzzeW6FhYWKiUlRfPmzat1//PPP69XXnlFCxYs0IYNGxQcHKwRI0aopKSkzmM6+jvfmOo736KiIm3atElPPvmkNm3apI8//lh79uzR9ddff87jOvK70FjOdW0l6eqrr65R97///e96j9lUr+25zvXMc0xPT9dbb70lk8mksWPH1nvcpnhdXcpAg1188cXG1KlT7e+tVquRkJBgzJkzp9b248aNM6655poa2wYMGGDcc889Lq3T2Y4fP25IMlauXFlnm4ULFxphYWGNV5QTzZo1y0hJSWlwe0+5roZhGA8++KDRvn17w2az1bq/uV5XScYnn3xif2+z2Yy4uDjjz3/+s31bTk6OYbFYjH//+991HsfR33l3+eX51mbjxo2GJOPQoUN1tnH0d8EdajvXCRMmGKNHj3boOM3h2jbkuo4ePdq48sor623THK6rs9Fz00BlZWX68ccfNWzYMPs2s9msYcOGad26dbV+Zt26dTXaS9KIESPqbN9U5ebmSpIiIyPrbVdQUKC2bdsqMTFRo0eP1s6dOxujPKfYu3evEhIS1K5dO91+++06fPhwnW095bqWlZXpnXfe0aRJk+pdRLY5X9dqqampysjIqHHdwsLCNGDAgDqv2/n8zjdlubm5MplMCg8Pr7edI78LTcmKFSsUExOjzp0767777lNWVladbT3l2mZmZurzzz/XnXfeec62zfW6ni/CTQOdPHlSVqtVsbGxNbbHxsYqIyOj1s9kZGQ41L4pstlsmj59ugYPHqwePXrU2a5z585666239Omnn+qdd96RzWbToEGDdOTIkUas9vwMGDBAixYt0pIlSzR//nylpqbqsssuU35+fq3tPeG6StLixYuVk5OjiRMn1tmmOV/XM1VfG0eu2/n8zjdVJSUleuyxx3TbbbfVu7Cio78LTcXVV1+tf/zjH1q+fLmee+45rVy5UiNHjpTVaq21vadc27ffflshISG68cYb623XXK/rhfC6VcHhmKlTp2rHjh3nvD87cOBADRw40P5+0KBB6tq1q/7+97/rD3/4g6vLvCAjR460f9+rVy8NGDBAbdu21QcffNCg/yNqrt58802NHDlSCQkJdbZpztcVlcrLyzVu3DgZhqH58+fX27a5/i7ceuut9u979uypXr16qX379lqxYoWGDh3qxspc66233tLtt99+zkH+zfW6Xgh6bhooOjpaPj4+yszMrLE9MzNTcXFxtX4mLi7OofZNzbRp0/TZZ5/p22+/VevWrR36rJ+fn/r06aN9+/a5qDrXCQ8PV6dOneqsvblfV0k6dOiQli1bprvuusuhzzXX61p9bRy5bufzO9/UVAebQ4cOaenSpfX22tTmXL8LTVW7du0UHR1dZ92ecG2/++477dmzx+HfYan5XldHEG4ayN/fX/369dPy5cvt22w2m5YvX17j/2zPNHDgwBrtJWnp0qV1tm8qDMPQtGnT9Mknn+ibb75RcnKyw8ewWq3avn274uPjXVChaxUUFGj//v111t5cr+uZFi5cqJiYGF1zzTUOfa65Xtfk5GTFxcXVuG55eXnasGFDndftfH7nm5LqYLN3714tW7ZMUVFRDh/jXL8LTdWRI0eUlZVVZ93N/dpKlT2v/fr1U0pKisOfba7X1SHuHtHcnLz33nuGxWIxFi1aZPz000/G5MmTjfDwcCMjI8MwDMO44447jMcff9zefs2aNYavr6/xl7/8xdi1a5cxa9Ysw8/Pz9i+fbu7TqFB7rvvPiMsLMxYsWKFkZ6ebn8VFRXZ2/zyXJ9++mnjq6++Mvbv32/8+OOPxq233moEBAQYO3fudMcpOOThhx82VqxYYaSmphpr1qwxhg0bZkRHRxvHjx83DMNzrms1q9VqtGnTxnjsscfO2tecr2t+fr6xefNmY/PmzYYkY+7cucbmzZvts4OeffZZIzw83Pj000+Nbdu2GaNHjzaSk5ON4uJi+zGuvPJK49VXX7W/P9fvvDvVd75lZWXG9ddfb7Ru3drYsmVLjd/j0tJS+zF+eb7n+l1wl/rONT8/33jkkUeMdevWGampqcayZcuMvn37Gh07djRKSkrsx2gu1/Zc/x0bhmHk5uYaQUFBxvz582s9RnO5rq5EuHHQq6++arRp08bw9/c3Lr74YmP9+vX2fZdffrkxYcKEGu0/+OADo1OnToa/v7/RvXt34/PPP2/kih0nqdbXwoUL7W1+ea7Tp0+3/7vExsYao0aNMjZt2tT4xZ+HW265xYiPjzf8/f2NVq1aGbfccouxb98++35Pua7VvvrqK0OSsWfPnrP2Nefr+u2339b63231+dhsNuPJJ580YmNjDYvFYgwdOvSsf4O2bdsas2bNqrGtvt95d6rvfFNTU+v8Pf7222/tx/jl+Z7rd8Fd6jvXoqIi46qrrjJatmxp+Pn5GW3btjXuvvvus0JKc7m25/rv2DAM4+9//7sRGBho5OTk1HqM5nJdXclkGIbh0q4hAACARsSYGwAA4FEINwAAwKMQbgAAgEch3AAAAI9CuAEAAB6FcAMAADwK4QYAAHgUwg0Ar2cymbR48WJ3lwHASQg3ANxq4sSJMplMZ72uvvpqd5cGoJnydXcBAHD11Vdr4cKFNbZZLBY3VQOguaPnBoDbWSwWxcXF1XhFRERIqrxlNH/+fI0cOVKBgYFq166dPvrooxqf3759u6688koFBgYqKipKkydPVkFBQY02b731lrp37y6LxaL4+HhNmzatxv6TJ0/qhhtuUFBQkDp27Kj//ve/rj1pAC5DuAHQ5D355JMaO3astm7dqttvv1233nqrdu3aJUkqLCzUiBEjFBERoe+//14ffvihli1bViO8zJ8/X1OnTtXkyZO1fft2/fe//1WHDh1q/Iynn35a48aN07Zt2zRq1CjdfvvtOnXqVKOeJwAncffKnQC824QJEwwfHx8jODi4xuuPf/yjYRiVq9Tfe++9NT4zYMAA47777jMMwzBee+01IyIiwigoKLDv//zzzw2z2WxfGTohIcH4/e9/X2cNkownnnjC/r6goMCQZHz55ZdOO08AjYcxNwDcbsiQIZo/f36NbZGRkfbvBw4cWGPfwIEDtWXLFknSrl27lJKSouDgYPv+wYMHy2azac+ePTKZTDp27JiGDh1abw29evWyfx8cHKzQ0FAdP378fE8JgBsRbgC4XXBw8Fm3iZwlMDCwQe38/PxqvDeZTLLZbK4oCYCLMeYGQJO3fv36s9537dpVktS1a1dt3bpVhYWF9v1r1qyR2WxW586dFRISoqSkJC1fvrxRawbgPvTcAHC70tJSZWRk1Njm6+ur6OhoSdKHH36o/v3769JLL9W//vUvbdy4UW+++aYk6fbbb9esWbM0YcIEzZ49WydOnND999+vO+64Q7GxsZKk2bNn695771VMTIxGjhyp/Px8rVmzRvfff3/jniiARkG4AeB2S5YsUXx8fI1tnTt31u7duyVVzmR67733NGXKFMXHx+vf//63unXrJkkKCgrSV199pQcffFAXXXSRgoKCNHbsWM2dO9d+rAkTJqikpEQvvviiHnnkEUVHR+umm25qvBME0KhMhmEY7i4CAOpiMpn0ySefaMyYMe4uBUAzwZgbAADgUQg3AADAozDmBkCTxp1zAI6i5wYAAHgUwg0AAPAohBsAAOBRCDcAAMCjEG4AAIBHIdwAAACPQrgBAAAehXADAAA8CuEGAAB4lP8PBYrZu2ctXkMAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 20\n",
      "Max: 2.3, epoch: 0\n",
      "Min: 0.12702270974046703, epoch: 19\n"
     ]
    }
   ],
   "source": [
    "loss_list = model_d.get(\"loss_list\")\n",
    "plt.figure()\n",
    "plt.title(\"CodeGen loss\")\n",
    "plt.ylabel(\"Cross-entropy loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.plot(loss_list)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Epochs: {len(loss_list)}\")\n",
    "print(f\"Max: {max(loss_list)}, epoch: {np.argmax(loss_list)}\")\n",
    "print(f\"Min: {min(loss_list)}, epoch: {np.argmin(loss_list)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "352155d0597c4d3d96bbf25a1ff595a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average target lines per sample: 5.300\n",
      "Average predic lines per sample: 22.008\n",
      "Average target sample size: 272.271\n",
      "Average predic sample size: 727.209\n"
     ]
    }
   ],
   "source": [
    "tgr_ls = 0\n",
    "tgr_cs = 0\n",
    "prd_ls = 0\n",
    "prd_cs = 0\n",
    "s = len(model_d.get(\"source_sentences\"))\n",
    "for tgr, prd in tqdm(zip(model_d.get(\"source_sentences\"), model_d.get(\"target_sentences\"))):\n",
    "    # tgr = tgr.replace(\";\", \";\\n\").replace(\"{\", \"{\\n\").replace(\"}\", \"}\\n\").replace(\"int\", \"\\nint\").replace(\"float\", \"\\nfloat\")\n",
    "    # prd = prd.replace(\";\", \";\\n\").replace(\"{\", \"{\\n\").replace(\"}\", \"}\\n\").replace(\"int\", \"\\nint\").replace(\"float\", \"\\nfloat\")\n",
    "    \n",
    "    tgr_ls += tgr.count(\"\\n\")\n",
    "    prd_ls += prd.count(\"\\n\")\n",
    "    \n",
    "    tgr_cs += len(tgr)\n",
    "    prd_cs += len(prd)\n",
    "    \n",
    "print(\"Average target lines per sample: {:.3f}\".format(tgr_ls/s))\n",
    "print(\"Average predic lines per sample: {:.3f}\".format(prd_ls/s))\n",
    "print(\"Average target sample size: {:.3f}\".format(tgr_cs/s))\n",
    "print(\"Average predic sample size: {:.3f}\".format(prd_cs/s))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE:\n",
      "// grid 1D block 1D\n",
      "\n",
      "__global__ void sumMatrixOnGPU1D(float *MatA, float *MatB, float *MatC, int nx,\n",
      "                                 int ny)\n",
      "\n",
      "TARGET:\n",
      "{\n",
      "    unsigned int ix = threadIdx.x + blockIdx.x * blockDim.x;\n",
      "\n",
      "    if (ix < nx )\n",
      "        for (int iy = 0; iy < ny; iy++)\n",
      "        {\n",
      "            int idx = iy * nx + ix;\n",
      "            MatC[idx] = MatA[idx] + MatB[idx];\n",
      "        }\n",
      "\n",
      "\n",
      "}\n",
      "\n",
      "PREDICT:\n",
      "\n",
      "{\n",
      "      unsigned int ix = threadIdx.x + blockIdx.x * blockDim.x;\n",
      "      if (ix < nx )          for (int iy = 0;\n",
      " iy < ny;\n",
      " iy++)          {\n",
      "              int idx = iy * nx + ix;\n",
      "              MatC[idx] = MatA[idx] + MatB[idx];\n",
      "          }\n",
      "\n",
      "\n",
      "}\n",
      "#--------------------------#\n"
     ]
    }
   ],
   "source": [
    "# Generate random example from evaluation\n",
    "samples = list(zip(model_d.get(\"source_sentences\"), model_d.get(\"target_sentences\"), model_d.get(\"predic_sentences\")))\n",
    "np.random.shuffle(samples)\n",
    "src_l, tgr_l, prd_l = zip(*samples)\n",
    "for src, tgr, prd in zip(src_l, tgr_l, prd_l):\n",
    "\n",
    "    if len(tgr) > 300:\n",
    "        continue\n",
    "    \n",
    "    print(\"SOURCE:\")\n",
    "    print(src)\n",
    "    print(\"\\nTARGET:\")\n",
    "    print(tgr)\n",
    "    print(\"\\nPREDICT:\")\n",
    "    print(prd)\n",
    "    print(\"#--------------------------#\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "// Kernel for dividing by two\n",
    "__device__ float divideByTwo(float v)\n",
    "\"\"\".strip()\n",
    "\n",
    "target = \"\"\"\n",
    "{\n",
    "    return v / 2;\n",
    "}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'ax_length': 500} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.239\n",
      " Function Function for operator by reduction\n",
      "//device__ void2ByTwo(float3, returnreturn f / f.\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "model = model.eval()\n",
    "x = tokenizer(prompt, max_length=500, padding=True, truncation=True, return_tensors=\"pt\").to(DEVICE)\n",
    "x[\"attention_mask\"] = x[\"attention_mask\"].type(torch.bool)\n",
    "y = tokenizer(target, max_length=500, padding=True, truncation=True, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "xy_str = f\"{prompt}{target}\"\n",
    "xy = tokenizer(xy_str, ax_length=500, padding=True, truncation=True, return_tensors=\"pt\").to(DEVICE)\n",
    "# xy[\"attention_mask\"] = xy[\"attention_mask\"].type(torch.bool)\n",
    "\n",
    "input_tokens = [token for token in tokenizer.convert_ids_to_tokens(x[\"input_ids\"][0], skip_special_tokens=True)]\n",
    "target_tokens = [token for token in tokenizer.convert_ids_to_tokens(y[\"input_ids\"][0], skip_special_tokens=True)]\n",
    "\n",
    "output = model(**xy, labels=xy[\"input_ids\"], output_attentions=True)\n",
    "pred = torch.argmax(output.logits, dim=-1)\n",
    "pred_tokens = [token for token in tokenizer.convert_ids_to_tokens(pred[0])]\n",
    "print(f\"Loss: {output.loss:.3f}\")\n",
    "print(tokenizer.batch_decode(pred, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mrandom\u001b[39;00m \u001b[39mimport\u001b[39;00m shuffle \n\u001b[0;32m----> 3\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      4\u001b[0m losses \u001b[39m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m sample \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from random import shuffle \n",
    "\n",
    "model = model.eval()\n",
    "losses = []\n",
    "sample = 1000\n",
    "l = list(zip(model_d.get(\"source_sentences\"), model_d.get(\"target_sentences\")))\n",
    "pred_sentences = []\n",
    "sample_size_loss_list = []\n",
    "shuffle(l)\n",
    "for i, (src, tgr) in tqdm(enumerate(l[:sample])):\n",
    "    xy = tokenizer(f\"{src}{tgr}\", max_length=700, padding=True, truncation=True, return_tensors=\"pt\").to(DEVICE)\n",
    "    xy[\"attention_mask\"] = xy[\"attention_mask\"].type(torch.bool)\n",
    "    \n",
    "    output = model(**xy, labels=xy[\"input_ids\"])\n",
    "    pred = torch.max(output.logits, dim=-1)[1][0]\n",
    "    \n",
    "    losses.append(output.loss)\n",
    "    pred_sentences.append(tokenizer.decode(pred, skip_special_tokens=True))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loss_list \u001b[39m=\u001b[39m losses\n\u001b[1;32m      2\u001b[0m input_len_list \u001b[39m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m target_len_list \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "loss_list = losses\n",
    "input_len_list = []\n",
    "target_len_list = []\n",
    "\n",
    "for (src, tgr), prd in zip(l[:sample], pred_sentences):\n",
    "    input_len_list.append(len(tokenizer.encode(src)))\n",
    "    target_len_list.append(len(tokenizer.encode(tgr)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression \n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def scatter_hist(x, y, ax, ax_histx, ax_histy, title):\n",
    "    # no labels\n",
    "    ax_histx.tick_params(axis=\"x\", labelbottom=False)\n",
    "    ax_histy.tick_params(axis=\"y\", labelleft=False)\n",
    "\n",
    "    # the scatter plot:\n",
    "    ax.scatter(x, y)\n",
    "    \n",
    "    regressor = LinearRegression() \n",
    "    regressor.fit(np.array(x).reshape((len(x),1)), y)\n",
    "    p_y = regressor.predict(np.array(x).reshape((len(x),1)))\n",
    "    corr, _ = pearsonr(x, y)\n",
    "    ax.plot(x, p_y, linewidth=0.75, color=\"orange\")\n",
    "    ax.text(max(x)*0.8, p_y[-1], f\"Pear. corr={corr:.2f}\")\n",
    "\n",
    "    # now determine nice limits by hand:\n",
    "    binwidth = 0.25\n",
    "    \n",
    "    xmax = np.max(np.abs(x))\n",
    "    xlim = (int(xmax/binwidth) + 1) * binwidth\n",
    "    xbins = np.linspace(-xlim, xlim + binwidth, 50)\n",
    "    \n",
    "    ymax = np.max(np.abs(y))\n",
    "    ylim = (int(ymax/binwidth) + 1) * binwidth\n",
    "    ybins = np.linspace(-ylim, ylim + binwidth, 50)\n",
    "\n",
    "    ax_histx.hist(x, bins=xbins)\n",
    "    ax_histx.set_title(title)\n",
    "    ax_histy.hist(y, bins=ybins, orientation='horizontal')\n",
    "\n",
    "def plot_scatter_with_dist(x, y, title, xlabel, ylabel):\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Start with a square Figure.\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    gs = fig.add_gridspec(2, 2,  width_ratios=(4, 1), height_ratios=(1, 4),\n",
    "                        left=0.1, right=0.9, bottom=0.1, top=0.9,\n",
    "                        wspace=0.05, hspace=0.05)\n",
    "    # Create the Axes.\n",
    "    ax = fig.add_subplot(gs[1, 0])\n",
    "    ax_histx = fig.add_subplot(gs[0, 0], sharex=ax)\n",
    "    ax_histy = fig.add_subplot(gs[1, 1], sharey=ax)\n",
    "    \n",
    "    min_x = np.quantile(x, 0.05)\n",
    "    max_x = np.quantile(x, 0.95)\n",
    "    min_y = np.quantile(y, 0.05)\n",
    "    max_y = np.quantile(y, 0.95)\n",
    "    \n",
    "    # x_mask = ((x > min_x) & (x < max_x))\n",
    "    # x = x[x_mask]\n",
    "    # y = y[x_mask]\n",
    "    # y_mask = ((y > min_y) & (y < max_y))\n",
    "    # x = x[y_mask]\n",
    "    # y = y[y_mask]\n",
    "    \n",
    "    # Draw the scatter plot and marginals.\n",
    "    scatter_hist(x, y, ax, ax_histx, ax_histy, title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    \n",
    "    ax.set_xlim(min_x*0.9, max_x*1.1)\n",
    "    ax.set_ylim(min_y*0.9, max_y*1.1)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "target_len_list = np.array(target_len_list)\n",
    "input_len_list = np.array(input_len_list)\n",
    "loss_list = np.array(loss_list)\n",
    "\n",
    "plot_scatter_with_dist(input_len_list, loss_list, \"Kernel input length & loss\", \"Input length\", \"Loss value\")\n",
    "plot_scatter_with_dist(target_len_list, loss_list, \"Kernel target length & loss\", \"Target length\", \"Loss value\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_sample_len = []\n",
    "tgr_sample_len = []\n",
    "\n",
    "for prd, (_, tgr) in zip(pred_sentences, l[len(pred_sentences):]):\n",
    "    prd_sample_len.append(len(prd))\n",
    "    tgr_sample_len.append(len(tgr))\n",
    "\n",
    "plot_scatter_with_dist(prd_sample_len, tgr_sample_len, \"Prediction & target length\", \"Predic. length\", \"Target length\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze use of indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_c = 0\n",
    "global_f = 0\n",
    "other_c = 0\n",
    "other_f = 0\n",
    "\n",
    "for src, prd in tqdm(list(zip(model_d.get(\"source_sentences\"), model_d.get(\"pred_sentences\")))):\n",
    "    one_line_src = src.replace(\"\\n\", \" \")\n",
    "    is_global = one_line_src.find(\"__global__\") != -1\n",
    "    \n",
    "    prd_lines = prd.splitlines()\n",
    "    found = False\n",
    "    for line in prd_lines:\n",
    "        if line.find(\"threadIdx.\") != -1 and line.find(\"blockIdx.\") != -1 and line.find(\"blockDim.\") != -1:\n",
    "            if is_global:\n",
    "                global_c += 1\n",
    "            else:\n",
    "                other_f += 1\n",
    "            found = True\n",
    "            break\n",
    "        \n",
    "    if not found:\n",
    "        if is_global:\n",
    "            global_f += 1\n",
    "        else:\n",
    "            other_c += 1\n",
    "\n",
    "print(f\"global_c: {global_c} ({global_c/(global_c+global_f):.2%})\")\n",
    "print(f\"global_f: {global_f} ({global_f/(global_c+global_f):.2%})\")\n",
    "print(f\"others_c: {other_c} ({other_c/(other_c+other_f):.2%})\")\n",
    "print(f\"others_f: {other_f} ({other_f/(other_c+other_f):.2%})\")\n",
    "\n",
    "total = global_c + global_f + other_c + other_f\n",
    "correct = global_c + other_c\n",
    "incorrect = global_f + other_f\n",
    "print(f\"Correct: {correct} ({correct/total:.2%})\")\n",
    "print(f\"Incorrect: {incorrect}, ({incorrect/total:.2%})\")\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use of local memory analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "t_p = 0\n",
    "t_n = 0\n",
    "f_p = 0\n",
    "f_n = 0\n",
    "\n",
    "t = []\n",
    "p = []\n",
    "\n",
    "for tgr, prd in tqdm(list(zip(model_d.get(\"target_sentences\"), model_d.get(\"pred_sentences\")))):\n",
    "    tgr_l_m_use = tgr.find(\"__shared__\") != -1 or tgr.find(\"__constant__\") != -1\n",
    "    prd_l_m_use = prd.find(\"__shared__\") != -1 or prd.find(\"__constant__\") != -1\n",
    "    \n",
    "    if tgr_l_m_use:\n",
    "        if prd_l_m_use:\n",
    "            t_p += 1\n",
    "        else:\n",
    "            f_n += 1\n",
    "    else:\n",
    "        if prd_l_m_use:\n",
    "            f_p += 1\n",
    "        else:\n",
    "            t_n += 1\n",
    "            \n",
    "    t.append(tgr_l_m_use)\n",
    "    p.append(prd_l_m_use)\n",
    "    \n",
    "total = t_p + t_n + f_p + f_n\n",
    "print(f\"True positive: {t_p}\\t({t_p/total:.2%})\")\n",
    "print(f\"True negative: {t_n}\\t({t_n/total:.2%})\")\n",
    "print(f\"False positive: {f_p}\\t({f_p/total:.2%})\")\n",
    "print(f\"False negative: {f_n}\\t({f_n/total:.2%})\")\n",
    "\n",
    "precision = precision_score(t, p)\n",
    "recall = recall_score(t, p)\n",
    "f1 = f1_score(t, p)\n",
    "\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1: {f1:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diplomka",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
